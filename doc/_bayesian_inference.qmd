---
editor:
    render-on-save: true
bibliography: references.bib
csl: ieee.csl
---
## Bayesian inference {#sec-bayesian_inference}

As defined in @sec-fitness_model, our ultimate objective is to infer the vector
of relative fitness values
$$
\underline{s}^M = (s^{(1)}, s^{(2)}, \ldots, s^{(M)})^\dagger,
$${#eq-fitness_mut_vec}
where $^\dagger$ indicates the transpose. Our data consists of an $T \times B$
matrix $\underline{\underline{R}}$, where $B = M + N$ is the number of unique
barcodes given by the sum of the number of unique relevant barcodes we care
about (hereafter referred to as mutant barcodes), $M$, and the number of unique
neutral barcodes, $N$, and $T$ is the number of time points where measurements
were taken. The data matrix is then of the form
$$
\underline{\underline{R}} = \begin{bmatrix}
- & \underline{r}_1 & - \\
- & \underline{r}_2 & - \\
 & \vdots & \\
- & \underline{r}_T & - \\
\end{bmatrix},
$${#eq-R_mat}
where each row $\underline{r}_t$ is a $B$-dimensional array containing the
raw barcode counts at time $t$. We can further split each vector
$\underline{r}_t$ into two vectors of the form
$$
\underline{r}_t = \begin{bmatrix}
\underline{r}_t^{N} \;
\underline{r}_t^{M}
\end{bmatrix},
$${#eq-r-vec_split}
i.e., the vector containing the neutral lineages barcode counts
$\underline{r}_t^{N}$ and the corresponding vector containing the mutant barcode
counts $\underline{r}_t^{M}$. Following the same logic, matrix
$\underline{\underline{R}}$ can be split into two matrices as
$$
\underline{\underline{R}} = \left[ 
\underline{\underline{R}}^N \; \underline{\underline{R}}^M
\right],
$${#eq-R-mat_split}
where $\underline{\underline{R}}^N$ is a $T \times N$ matrix with the
barcode reads time series for each neutral lineage and
$\underline{\underline{R}}^M$ is the equivalent $T \times M$ matrix for the
mutant lineages.


Our objective is to compute the joint probability
distribution for all relative fitness values given our data. We can express this 
joint posterior distribution using Bayes theorem as
$$
\pi(\underline{s}^M \mid \underline{\underline{R}}) = \frac{
\pi(\underline{\underline{R}} \mid \underline{s}^M) 
\pi(\underline{s}^M)}
{\pi(\underline{\underline{R}})},
$${#eq-bayes_obj}
where hereafter $\pi(\cdot)$ defines a probability density function, unless
otherwise stated. To develop our model, we can ignore the denominator on the
right-hand side of @eq-bayes_obj, since it serves as a normalization constant
that does not depend on our parameter of interest. However, computing this 
normalization constant represents a major challenge we will have to overcome. 
For now, we can simply write
$$
\pi(\underline{s}^M \mid \underline{\underline{R}}) \propto
\pi(\underline{\underline{R}} \mid \underline{s}^M) 
\pi(\underline{s}^M)
$${#eq-bayes_obj_propto}

Although @eq-bayes_obj and @eq-bayes_obj_propto seem simple enough, recall that
@eq-fitness relates barcode frequency values and the population mean fitness to
the mutant relative fitness. Therefore, we must include these nuisance
parameters as part of our inference problem. We can always marginalize the joint
distribution to obtain the distribution we are interested in. To include these
parameters, let 
$$
\underline{\bar{s}}_T = (\bar{s}_1, \bar{s}_2, \ldots, \bar{s}_{T-1})^\dagger,
$${#eq-pop_fitness_vec}
be the vector containing the $T-1$ population mean fitness we compute from the
$T$ time points where measurements were taken. We have $T-1$ since the value
of any $\bar{s}_t$ requires time points $t$ and $t+1$. Furthermore, let the
matrix $\underline{\underline{F}}$ be a $T \times B$ matrix containing all
frequency values. As with @eq-R_mat, we can split $\underline{\underline{F}}$
into two matrices of the form
$$
\underline{\underline{F}} = \left[ 
\underline{\underline{F}}^N \; \underline{\underline{F}}^M
\right],
$${#eq-F-mat_split}
to separate the corresponding mutant and neutral barcode frequencies. With these
nuisance variables in hand, the full inference problem we must solve takes the
form
$$
\pi(
    \underline{s}^M, \underline{\bar{s}}_T, \underline{\underline{F}} \mid
    \underline{\underline{R}}
) \propto
\pi(
    \underline{\underline{R}} \mid
    \underline{s}^M, \underline{\bar{s}}_T, \underline{\underline{F}}
)
\pi(
    \underline{s}^M, \underline{\bar{s}}_T, \underline{\underline{F}}
).
$${#eq-bayes_full}
We can numerically integrate out all nuisance parameters to recover the marginal
distribution we care about, i.e.,
$$
\pi(\underline{s}^M \mid \underline{\underline{R}}) =
\int d^{T-1}\underline{\bar{s}}_T
\int d^{B}\underline{f}_1 \cdots
\int d^{B}\underline{f}_T
\;
\pi(
    \underline{s}^M, \underline{\bar{s}}_T, \underline{\underline{F}} \mid
    \underline{\underline{R}}
).
$${#eq-bayes_full_marginal}

### Factorizing the posterior distribution {#seq-split_posterior}

The left-hand side of @eq-bayes_full is extremely difficult to work with.
However, we can take advantage of the structure of our inference problem to
rewrite it in a more manageable form. Specifically, as depicted in @fig-01(C),
the statistical dependencies of our observations and latent variables allows us
to rewrite the joint probability distribution as a product of conditional
distributions of the form
$$
\pi(
    \underline{s}^M, \underline{\bar{s}}_T, \underline{\underline{F}} \mid
    \underline{\underline{R}}
) =
\pi(
    \underline{s}^M \mid \underline{\bar{s}}_T, \underline{\underline{F}}^M,
    \underline{\underline{R}}^M
)
\pi(
    \underline{\bar{s}}_T \mid \underline{\underline{F}}^N,
    \underline{\underline{R}}^N
)
\pi(\underline{\underline{F}} \mid \underline{\underline{R}}).
$${#eq-split_posterior}
Written in this form, @eq-split_posterior captures the three sources of 
uncertainty listed in @sec-fitness_model in each of the terms. Starting from
right to left, the first term on the right-hand side of @eq-split_posterior
accounts for the uncertainty when inferring the frequency values given the
barcode reads. The second term accounts for the uncertainty in the values of
the mean population fitness at different time points. The last term accounts for
the uncertainty in the parameter we care about---the mutants' relative
fitnesses. We invite the reader to check the supplementary material where we
work through each of these terms to build the posterior distribution to sample
from. We refer the reader to `[ref SI section]` for an extended description of
the model with specific functional forms for each term on the left-hand side of
@eq-split_posterior as well as `[ref SI on hierarchical]` where the model is
extended to account for multiple experimental replicates.

### Variational Inference

One of the technical challenges to the adoption of Bayesian methods is the
analytical intractability of integrals such as that of @eq-bayes_full_marginal.
Furthermore, even though efficient Markov Chain Monte Carlo (MCMC) algorithms
such as Hamiltonian Montecarlo can numerically perform this integration
[@betancourt2017], the dimensionality of the problem in @eq-split_posterior makes
an MCMC-based approach prohibitively slow.

To overcome this computational limitation, in this work, we rely on the recent
development of the automatic differentiation variational inference algorithm
(ADVI) [@kucukelbir2016]. Briefly, when performing ADVI, our target posterior
distribution $\pi(\theta \mid \underline{\underline{R}})$, where $\theta =
(\underline{s}^M, \underline{\bar{s}}_T, \underline{\underline{F}})$, is
replaced by an approximate posterior distribution $q_\phi(\theta \mid
\underline{\underline{R}})$, where $\phi$ fully parametrizes the approximate
distribution. As further explained in `[point to SI section]`, the numerical
integration problem is replaced by an optimization problem of the form
$$
q^*_\phi(\theta \mid \underline{\underline{R}}) = \min _\phi
D_{KL}(
    q_\phi(\theta \mid \underline{\underline{R}}) \lvert \lvert
    \pi(\theta \mid \underline{\underline{R}})
),
$${#eq-vi_objective}
where $D_{KL}$ is the Kulback-Leibler divergence. In other words, the 
complicated high-dimensional numerical integration problem is transformed into
a gradient descent over the parameters of the approximate posterior distribution
$\phi$ such that it minimizes the KL divergence with respect to the true 
posterior distribution. Although to compute @eq-vi_objective we require the
posterior distribution we are trying to approximate $\pi(\theta \mid
\underline{\underline{R}})$, it can be shown that maximizing the so-called
evidence lower bound (ELBO) [@kingma2014]---equivalent to minimizing the 
variational free energy [@buckley2017]---is mathematically equivalent to 
performing the optimization prescribed by @eq-vi_objective. We direct the reader
to `[point to SI section]` for a short primer on variational inference.

This work is accompanied by the Julia library `BayesFitness.jl` that makes use
of the implementation of both MCMC-based integration as well as ADVI
optimization to numerically approximate the solution of @eq-bayes_full_marginal
within the Julia ecosystem [@ge2018].