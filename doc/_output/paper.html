<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Manuel Razo-Mejia">
<meta name="author" content="Madhav Mani">
<meta name="author" content="Dmitri Petrov">
<meta name="keywords" content="bayesian inference, variational inference, relative fitness, open-source, reproducible research">

<title>Bayesian inference of relative fitness on high-throughput pooled competition assays</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    <img src="./logo.png" alt="" class="navbar-logo">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://mrazomej.github.io/BarBay.jl" rel="" target="">
 <span class="menu-text">BarBay.jl</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/mrazomej/bayesian_fitness" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/mrazomej" rel="" target=""><i class="bi bi-twitter-x" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Bayesian inference of relative fitness on high-throughput pooled competition assays</li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a>
  <ul class="collapse">
  <li><a href="#sec-experiment" id="toc-sec-experiment" class="nav-link" data-scroll-target="#sec-experiment">Experimental setup</a></li>
  <li><a href="#preliminaries-on-mathematical-notation" id="toc-preliminaries-on-mathematical-notation" class="nav-link" data-scroll-target="#preliminaries-on-mathematical-notation">Preliminaries on mathematical notation</a></li>
  <li><a href="#sec-fitness_model" id="toc-sec-fitness_model" class="nav-link" data-scroll-target="#sec-fitness_model">Fitness model</a></li>
  <li><a href="#sec-bayesian_inference" id="toc-sec-bayesian_inference" class="nav-link" data-scroll-target="#sec-bayesian_inference">Bayesian inference</a>
  <ul class="collapse">
  <li><a href="#seq-split_posterior" id="toc-seq-split_posterior" class="nav-link" data-scroll-target="#seq-split_posterior">Factorizing the posterior distribution</a></li>
  <li><a href="#variational-inference" id="toc-variational-inference" class="nav-link" data-scroll-target="#variational-inference">Variational Inference</a></li>
  </ul></li>
  <li><a href="#inference-on-a-single-dataset" id="toc-inference-on-a-single-dataset" class="nav-link" data-scroll-target="#inference-on-a-single-dataset">Inference on a single dataset</a></li>
  <li><a href="#sec-multienv" id="toc-sec-multienv" class="nav-link" data-scroll-target="#sec-multienv">Fitness inference on multiple environments</a></li>
  <li><a href="#sec-replicates" id="toc-sec-replicates" class="nav-link" data-scroll-target="#sec-replicates">Accounting for experimental replicates via hierarchical models</a></li>
  <li><a href="#sec-genotypes" id="toc-sec-genotypes" class="nav-link" data-scroll-target="#sec-genotypes">Accounting for multiple barcodes per genotype via hierarchical models</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a></li>
  <li><a href="#acknowledgements" id="toc-acknowledgements" class="nav-link" data-scroll-target="#acknowledgements">Acknowledgements</a></li>
  <li><a href="#supplementary-materials" id="toc-supplementary-materials" class="nav-link" data-scroll-target="#supplementary-materials">Supplementary Materials</a>
  <ul class="collapse">
  <li><a href="#sec-vi_primer" id="toc-sec-vi_primer" class="nav-link" data-scroll-target="#sec-vi_primer">Primer on Variational Inference</a>
  <ul class="collapse">
  <li><a href="#advi-algorithm" id="toc-advi-algorithm" class="nav-link" data-scroll-target="#advi-algorithm">ADVI algorithm</a></li>
  </ul></li>
  <li><a href="#sec-bayes_def" id="toc-sec-bayes_def" class="nav-link" data-scroll-target="#sec-bayes_def">Defining the Bayesian model</a>
  <ul class="collapse">
  <li><a href="#sec-bayes_freq" id="toc-sec-bayes_freq" class="nav-link" data-scroll-target="#sec-bayes_freq">Frequency uncertainty <span class="math inline">\(\pi(\underline{\underline{F}} \mid \underline{\underline{R}})\)</span></a></li>
  <li><a href="#sec-bayes_meanfit" id="toc-sec-bayes_meanfit" class="nav-link" data-scroll-target="#sec-bayes_meanfit">Population mean fitness uncertainty <span class="math inline">\(\pi(\underline{\bar{s}}_T \mid \underline{\underline{F}}, \underline{\underline{R}})\)</span></a></li>
  <li><a href="#sec-bayes_mutfit" id="toc-sec-bayes_mutfit" class="nav-link" data-scroll-target="#sec-bayes_mutfit">Mutant relative fitness uncertainty <span class="math inline">\(\pi(\underline{s}^M \mid \underline{\bar{s}}_T, \underline{\underline{F}}, \underline{\underline{R}})\)</span></a></li>
  <li><a href="#sec-hierarchical_model" id="toc-sec-hierarchical_model" class="nav-link" data-scroll-target="#sec-hierarchical_model">Hierarchical models for multiple experimental replicates</a></li>
  <li><a href="#defining-prior-probabilities" id="toc-defining-prior-probabilities" class="nav-link" data-scroll-target="#defining-prior-probabilities">Defining prior probabilities</a></li>
  </ul></li>
  <li><a href="#sec-ppc" id="toc-sec-ppc" class="nav-link" data-scroll-target="#sec-ppc">Posterior predictive checks</a></li>
  <li><a href="#sec-logistic" id="toc-sec-logistic" class="nav-link" data-scroll-target="#sec-logistic">Logistic growth simulation</a></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="paper.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Bayesian inference of relative fitness on high-throughput pooled competition assays</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Manuel Razo-Mejia <a href="https://orcid.org/0000-0002-9510-0527" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Department of Biology, Stanford University
          </p>
      </div>
      <div class="quarto-title-meta-contents">
    <p class="author">Madhav Mani <a href="https://orcid.org/0000-0002-5812-4167" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            NSF-Simons Center for Quantitative Biology, Northwestern University
          </p>
        <p class="affiliation">
            Department of Molecular Biosciences, Northwestern University
          </p>
      </div>
      <div class="quarto-title-meta-contents">
    <p class="author">Dmitri Petrov <a href="https://orcid.org/0000-0002-3664-9130" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Department of Biology, Stanford University
          </p>
        <p class="affiliation">
            Stanford Cancer Institute, Stanford University School of Medicine
          </p>
        <p class="affiliation">
            Chan Zuckerberg Biohub
          </p>
      </div>
    </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="abstract-title">Abstract</div>
    <p>The tracking of lineage frequencies via DNA barcode sequencing enables the quantification of microbial fitness. However, experimental noise coming from biotic and abiotic sources complicates the computation of a reliable inference. We present a Bayesian pipeline to infer relative microbial fitness from high-throughput lineage tracking assays. Our model accounts for multiple sources of noise and propagates uncertainties throughout all parameters in a systematic way. Furthermore, using modern variational inference methods based on automatic differentiation, we are able to scale the inference to a large number of unique barcodes. We extend this core model to analyze multi-environment assays, replicate experiments, and barcodes linked to genotypes. On simulations, our method recovers known parameters within posterior credible intervals. This work provides a generalizable Bayesian framework to analyze lineage tracking experiments. The accompanying open-source software library enables the adoption of principled statistical methods in experimental evolution.</p>
  </div>
</div>

</header>

<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>The advent of DNA barcoding—the ability to uniquely identify cell lineages with DNA sequences integrated at a specific locus—and high-throughput sequencing has opened new venues for understanding microbial evolutionary dynamics with an unprecedented level of temporal resolution <span class="citation" data-cites="levy2015 nguyenba2019a ascensao2023"><a href="#ref-levy2015" role="doc-biblioref">[1]</a>–<a href="#ref-ascensao2023" role="doc-biblioref">[3]</a></span>. These experimental efforts rely on our ability to reliably infer the relative fitness of an ensemble of diverse genotypes. Moreover, inferring these fitness values over an ensemble of environmental conditions can help us determine the phenotypic diversity of a rapid adaptation process <span class="citation" data-cites="kinsler2020"><a href="#ref-kinsler2020" role="doc-biblioref">[4]</a></span>.</p>
<p>As with any other sequencing-based quantification, tracking lineages via DNA barcode sequencing is inexorably accompanied by noise sources coming from experimental manipulation of the microbial cultures, DNA extraction, and sequencing library preparation that involves multiple rounds of PCR amplification, and the sequencing process itself. Thus, accounting for the uncertainty when inferring the relevant parameters from the data is a crucial step to draw reliable conclusions. Bayesian statistics presents a paradigm by which one can account for all known sources of uncertainty in a principled way <span class="citation" data-cites="eddy2004a"><a href="#ref-eddy2004a" role="doc-biblioref">[5]</a></span>. This, combined with the development of modern Markov Chain Monte Carlo sampling algorithms <span class="citation" data-cites="betancourt2017"><a href="#ref-betancourt2017" role="doc-biblioref">[6]</a></span> and approximate variational approaches <span class="citation" data-cites="kucukelbir2016"><a href="#ref-kucukelbir2016" role="doc-biblioref">[7]</a></span> have boosted a resurgence of Bayesian methods in different fields <span class="citation" data-cites="efron2013a"><a href="#ref-efron2013a" role="doc-biblioref">[8]</a></span>.</p>
<p>We present a Bayesian inference pipeline to quantify the uncertainty about the parametric information we can extract from high-throughput competitive fitness assays given a model of the data generation process and experimental data. In these assays, the fitness of an ensemble of genotypes is determined relative to a reference genotype <span class="citation" data-cites="kinsler2020 ascensao2023"><a href="#ref-ascensao2023" role="doc-biblioref">[3]</a>, <a href="#ref-kinsler2020" role="doc-biblioref">[4]</a></span>. <a href="#fig-01">Figure&nbsp;1</a>(A) shows a schematic of the experimental procedure in which an initial pool of barcoded strains are mixed with a reference strain and inoculated into fresh media. After some time—usually, enough time for the culture to saturate—an aliquot is transferred to fresh media, while the remaining culture is used for DNA sequencing of the lineage barcodes. The time-series information of the relative abundance of each lineage, i.e., the barcode frequency depicted in <a href="#fig-01">Figure&nbsp;1</a>(B), is used to infer the relative fitness—the growth advantage on a per-cycle basis—for each lineage with respect to the reference strain. The proposed statistical model accounts for multiple sources of uncertainty when inferring the lineages’ relative fitness values (see <a href="#sec-experiment">Section&nbsp;2.1</a> for details on sources of uncertainty accounted for by the model). Furthermore, minor changes to the core statistical model allow us to account for relevant experimental variations of these competition assays. More specifically, in <a href="#sec-multienv">Section&nbsp;2.6</a>, we present a variation of the statistical model to infer fitness on growth dilution cycles in multiple environments with proper error propagation. Furthermore, as described in <a href="#sec-replicates">Section&nbsp;2.7</a>, our statistical model can account for batch-to-batch differences when jointly analyzing multiple experimental replicates using a Bayesian hierarchical model. Finally, a variant of these hierarchical models, presented in <a href="#sec-genotypes">Section&nbsp;2.8</a>, can account for variability within multiple barcodes mapping to equivalent genotypes within the same experiment.</p>
<p>For all the model variations presented in this paper, we benchmark the ability of our pipeline to infer relative fitness parameters against synthetic data generated from logistic growth simulations with added random noise. A <code>Julia</code> package accompanies the present method to readily implement the inference pipeline with state-of-the-art scientific computing software.</p>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<!-- Experimental results -->
<section id="sec-experiment" class="level2">
<h2 class="anchored" data-anchor-id="sec-experiment">Experimental setup</h2>
<p>The present work is designed to analyze time-series data of relative abundance of multiple microbial lineages uniquely identified by a DNA barcode <span class="citation" data-cites="kinsler2020 ascensao2023"><a href="#ref-ascensao2023" role="doc-biblioref">[3]</a>, <a href="#ref-kinsler2020" role="doc-biblioref">[4]</a></span>. In these competition assays, an ensemble of genotypes is pooled together with an unlabeled reference strain that, initially, represents the vast majority (<span class="math inline">\(\geq 90\%\)</span>) of the cells in the culture (see schematic in <a href="#fig-01">Figure&nbsp;1</a>(A)). Furthermore, a fraction of labeled genotypes equivalent to the unlabeled reference strain—hereafter defined as <em>neutral</em> lineages—are spiked in at a relatively high abundance (<span class="math inline">\(\approx 3-5\%\)</span>). The rest of the culture is left for the ensemble of genotypes of interest.</p>
<p>To determine the relative fitness of the ensemble of genotypes, a series of growth-dilution cycles are performed on either a single or multiple environments. In other words, the cultures are grown for some time; then, an aliquot is inoculated into fresh media for the next growth cycle. This process is repeated for roughly 4-7 cycles, depending on the initial abundances of the mutants and their relative growth rates. The DNA barcodes are sequenced at the end of each growth cycle to quantify the relative abundance of each of the barcodes. We point the reader to <span class="citation" data-cites="kinsler2020"><a href="#ref-kinsler2020" role="doc-biblioref">[4]</a></span> for specific details on these assays for <em>S. cerevisiae</em> and to <span class="citation" data-cites="ascensao2023"><a href="#ref-ascensao2023" role="doc-biblioref">[3]</a></span> for equivalent assays for <em>E. coli</em>. <a href="#fig-01">Figure&nbsp;1</a>(B) presents a typical barcode trajectory where the black trajectories represent the so-called <em>neutral lineages</em>, genetically equivalent to the untagged ancestor strain that initially dominates the culture. These spiked-in neutral lineages simplify the inference problem since the fitness metric of all relevant barcodes is quantified with respect to these barcodes—thus referred to as <em>relative fitness</em>.</p>
<div id="fig-01" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./figs/fig01.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;1: <strong>Typical competitive fitness experiment</strong>. (A) Schematic of the typical experimental design to determine the competitive fitness of an ensemble of barcoded genotypes. Genotypes are pooled together and grown over multiple growth-dilution cycles. At the end of each cycle, a sample is processed to generate a library for amplicon sequencing. (B) Typical barcode trajectory dataset. From each time point, the relative frequency of each barcode is determined from the total number of reads. Shades of blue represent different relative fitness. Darker gray lines define the typical trajectory of neutral lineages.</figcaption>
</figure>
</div>
<!-- Fitness model -->
</section>
<section id="preliminaries-on-mathematical-notation" class="level2">
<h2 class="anchored" data-anchor-id="preliminaries-on-mathematical-notation">Preliminaries on mathematical notation</h2>
<p>Before jumping directly into the Bayesian inference pipeline, let us establish the mathematical notation used throughout this paper. We define (column) vectors as underlined lowercase symbols such as <span id="eq-vec_def"><span class="math display">\[
\underline{x} = \begin{bmatrix}
    x_1\\
    x_2\\
    \vdots\\
    x_N
\end{bmatrix}.
\tag{1}\]</span></span> In the same way, we define matrices as double-underline uppercase symbols such as <span id="eq-mat_def"><span class="math display">\[
\underline{\underline{A}} =
\begin{bmatrix}
    A_{11} &amp; A_{12} &amp; \cdots &amp; A_{1N}\\
    A_{21} &amp; A_{22} &amp; \cdots &amp; A_{2N}\\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
    A_{M1} &amp; A_{M2} &amp; \cdots &amp; A_{MN}\\
\end{bmatrix}.
\tag{2}\]</span></span></p>
</section>
<section id="sec-fitness_model" class="level2">
<h2 class="anchored" data-anchor-id="sec-fitness_model">Fitness model</h2>
<p>Empirically, each barcode frequency trajectory follows an exponential function of the form <span class="citation" data-cites="levy2015 kinsler2020 ascensao2023"><a href="#ref-levy2015" role="doc-biblioref">[1]</a>, <a href="#ref-ascensao2023" role="doc-biblioref">[3]</a>, <a href="#ref-kinsler2020" role="doc-biblioref">[4]</a></span> <span id="eq-fitness"><span class="math display">\[
f_{t+1}^{(b)} = f_{t}^{(b)} \mathrm{e}^{(s^{(b)} - \bar{s}_t)\tau},
\tag{3}\]</span></span> where <span class="math inline">\(f_{t}^{(b)}\)</span> is the frequency of barcode <span class="math inline">\(b\)</span> at the end of cycle number <span class="math inline">\(t\)</span>, <span class="math inline">\(s^{(b)}\)</span> is the relative fitness with respect to the reference strain—the quantity we want to infer from the data—<span class="math inline">\(\bar{s}_t\)</span> is the mean fitness of the culture at the end of cycle number <span class="math inline">\(t\)</span>, and <span class="math inline">\(\tau\)</span> is the time pass between cycle <span class="math inline">\(t\)</span> and <span class="math inline">\(t+1\)</span>. We can rewrite <a href="#eq-fitness">Equation&nbsp;3</a> as <span id="eq-logfreq"><span class="math display">\[
\frac{1}{\tau}\ln \frac{f_{t+1}^{(b)}}{f_{t}^{(b)}} = (s^{(b)} - \bar{s}_t).
\tag{4}\]</span></span> <a href="#eq-logfreq">Equation&nbsp;4</a> separates the measurements—the barcode frequencies—from the unobserved (sometimes referred to as latent) parameters we want to infer from the data—the population mean fitness and the barcode relative fitness. This is ultimately the functional form used in our inference pipeline. Therefore, the relative fitness is computed by knowing the log frequency ratio of each barcode throughout the growth-dilution cycles.</p>
<p>The presence of the neutral lineages facilitates the determination of the population mean fitness value <span class="math inline">\(\bar{s}_t\)</span>. Since every relative fitness is determined relative to the neutral lineage that dominates the culture, we define their fitness to be <span class="math inline">\(s^{(n)} = 0\)</span>, where the superscript <span class="math inline">\((n)\)</span> specifies their neutrality. This means that <a href="#eq-logfreq">Equation&nbsp;4</a> for a neutral lineage takes the simpler form <span id="eq-logfreq_neutral"><span class="math display">\[
\frac{1}{\tau}\ln \frac{f_{t+1}^{(n)}}{f_{t}^{(n)}} = - \bar{s}_t.
\tag{5}\]</span></span> Therefore, we can use the data from these reference barcodes to directly infer the value of the population mean fitness.</p>
<p>It is important to notice that the frequencies <span class="math inline">\(f_{t}^{(b)}\)</span> are not the allele frequencies in the population (most of the culture is not sequenced since the reference strain is not barcoded), but rather the relative frequencies in the total number of sequencing reads. A way to conceptualize this subtle but important point is to assume exponential growth in the <em>number of cells</em> <span class="math inline">\(N_t^{(b)}\)</span> of the form <span id="eq-ncells"><span class="math display">\[
N_{t+1}^{(b)} = N_{t}^{(b)} \mathrm{e}^{\lambda^{(b)}\tau},
\tag{6}\]</span></span> for every barcode <span class="math inline">\(b\)</span> with growth rate <span class="math inline">\(\lambda^{(b)}\)</span>. However, when we sequence barcodes, we do not directly measure the number of cells, but some number of reads <span class="math inline">\(r_t^{(b)}\)</span> that map to barcode <span class="math inline">\(b\)</span>. In the simplest possible scenario, we assume <span id="eq-r_to_n"><span class="math display">\[
r_{t}^{(b)} \propto N_{t}^{(b)},
\tag{7}\]</span></span> where, importantly, the proportionality constant depends on the total number of reads for the library for cycle <span class="math inline">\(t\)</span>, which might vary from library to library. Therefore, to compare the number of reads between libraries at different time points, we must normalize the number of reads to the same scale. The simplest form is to define a relative abundance, i.e., a frequency with respect to the total number of reads, <span id="eq-r_to_n_norm"><span class="math display">\[
f_{t}^{(b)} \equiv \frac{r_{t}^{(b)}}{\sum_{b'} r_{t}^{(b')}}.
\tag{8}\]</span></span> This is the frequency <a href="#eq-fitness">Equation&nbsp;3</a> describes.</p>
<p>Our ultimate objective is to infer the relative fitness <span class="math inline">\(s^{(b)}\)</span> for each of the <span class="math inline">\(M\)</span> relevant barcodes in the experiment—hereafter referred to as <span class="math inline">\(s^{(m)}\)</span> to distinguish from the general <span class="math inline">\(s^{(b)}\)</span> and the neutral lineages <span class="math inline">\(s^{(n)}\)</span> relative fitness. To do so, we account for the three primary sources of uncertainty in our model:</p>
<ol type="1">
<li>Uncertainty in the determination of frequencies. Our model relates frequencies between adjacent growth-dilution cycles to the fitness of the corresponding strain. However, we do not directly measure frequencies. Instead, our data for each barcode consists of a length <span class="math inline">\(T\)</span> vector of counts <span class="math inline">\(\underline{r}^{(b)}\)</span> for each of the <span class="math inline">\(T\)</span> cycles in which the measurements were taken.</li>
<li>Uncertainty in the value of the population mean fitness. We define neutral lineages to have fitness <span class="math inline">\(s^{(n)} = 0\)</span>, helping us anchor the value of the population mean fitness <span class="math inline">\(\bar{s}_t\)</span> for each pair of adjacent growth cycles. Moreover, we take this parameter as an empirical parameter to be obtained from the data, meaning that we do not impose a functional form that relates <span class="math inline">\(\bar{s}_t\)</span> to <span class="math inline">\(\bar{s}_{t+1}\)</span>. Thus, we must infer the <span class="math inline">\(T-1\)</span> values of this population mean fitness with their uncertainty that must be propagated to the value of the mutants’ relative fitness.</li>
<li>Uncertainty in each of the mutants’ fitness values.</li>
</ol>
<p>To account for all these sources of uncertainty in a principled way, in the next section, we develop a Bayesian inference pipeline.</p>
<!-- Bayesian inference -->
</section>
<section id="sec-bayesian_inference" class="level2">
<h2 class="anchored" data-anchor-id="sec-bayesian_inference">Bayesian inference</h2>
<p>As defined in <a href="#sec-fitness_model">Section&nbsp;2.3</a>, our ultimate objective is to infer the vector of relative fitness values <span id="eq-fitness_mut_vec"><span class="math display">\[
\underline{s}^M = (s^{(1)}, s^{(2)}, \ldots, s^{(M)})^\dagger,
\tag{9}\]</span></span> where <span class="math inline">\(^\dagger\)</span> indicates the transpose. Our data consists of an <span class="math inline">\(T \times B\)</span> matrix <span class="math inline">\(\underline{\underline{R}}\)</span>, where <span class="math inline">\(B = M + N\)</span> is the number of unique barcodes given by the sum of the number of unique, relevant barcodes we care about, <span class="math inline">\(M\)</span>, and the number of unique neutral barcodes, <span class="math inline">\(N\)</span>, and <span class="math inline">\(T\)</span> is the number of growth cycles where measurements were taken. The data matrix is then of the form <span id="eq-R_mat"><span class="math display">\[
\underline{\underline{R}} = \begin{bmatrix}
- &amp; \underline{r}_1 &amp; - \\
- &amp; \underline{r}_2 &amp; - \\
&amp; \vdots &amp; \\
- &amp; \underline{r}_T &amp; - \\
\end{bmatrix},
\tag{10}\]</span></span> where each row <span class="math inline">\(\underline{r}_t\)</span> is a <span class="math inline">\(B\)</span>-dimensional array containing the raw barcode counts at cycle <span class="math inline">\(t\)</span>. We can further split each vector <span class="math inline">\(\underline{r}_t\)</span> into two vectors of the form <span id="eq-r-vec_split"><span class="math display">\[
\underline{r}_t = \begin{bmatrix}
\underline{r}_t^{N} \\
\underline{r}_t^{M}
\end{bmatrix},
\tag{11}\]</span></span> i.e., the vector containing the neutral lineage barcode counts <span class="math inline">\(\underline{r}_t^{N}\)</span> and the corresponding vector containing the mutant barcode counts <span class="math inline">\(\underline{r}_t^{M}\)</span>. Following the same logic, matrix <span class="math inline">\(\underline{\underline{R}}\)</span> can be split into two matrices as <span id="eq-R-mat_split"><span class="math display">\[
\underline{\underline{R}} = \left[
\underline{\underline{R}}^N \; \underline{\underline{R}}^M
\right],
\tag{12}\]</span></span> where <span class="math inline">\(\underline{\underline{R}}^N\)</span> is a <span class="math inline">\(T \times N\)</span> matrix with the barcode reads time series for each neutral lineage and <span class="math inline">\(\underline{\underline{R}}^M\)</span> is the equivalent <span class="math inline">\(T \times M\)</span> matrix for the non-neutral lineages.</p>
<p>Our objective is to compute the joint probability distribution for all relative fitness values given our data. We can express this joint posterior distribution using Bayes theorem as <span id="eq-bayes_obj"><span class="math display">\[
\pi(\underline{s}^M \mid \underline{\underline{R}}) = \frac{
\pi(\underline{\underline{R}} \mid \underline{s}^M)
\pi(\underline{s}^M)}
{\pi(\underline{\underline{R}})},
\tag{13}\]</span></span> where hereafter <span class="math inline">\(\pi(\cdot)\)</span> defines a probability density function. When defining our statistical model, we need not to focus on the denominator on the right-hand side of <a href="#eq-bayes_obj">Equation&nbsp;13</a>. Thus, we can write <span id="eq-bayes_obj_propto"><span class="math display">\[
\pi(\underline{s}^M \mid \underline{\underline{R}}) \propto
\pi(\underline{\underline{R}} \mid \underline{s}^M)
\pi(\underline{s}^M).
\tag{14}\]</span></span> However, when implementing the model computationally, the normalization constant on the right-hand side of <a href="#eq-bayes_obj">Equation&nbsp;13</a> must be computed. This can be done from the definition of the model via an integral of the form <span id="eq-bayes_evidence"><span class="math display">\[
\pi(\underline{\underline{R}}) = \int d^M \underline{s}^M
\pi(\underline{\underline{R}} \mid \underline{s}^M)
\pi(\underline{s}^M),
\tag{15}\]</span></span> also known as a marginalization integral. Hereafter, differentials of the form <span class="math inline">\(d^n\)</span> imply a <span class="math inline">\(n\)</span>-dimensional integral.</p>
<p>Although <a href="#eq-bayes_obj">Equation&nbsp;13</a> and <a href="#eq-bayes_obj_propto">Equation&nbsp;14</a> seem simple enough, recall that <a href="#eq-fitness">Equation&nbsp;3</a> relates barcode frequency values and the population mean fitness to the mutant relative fitness. Therefore, we must include these nuisance parameters as part of our inference problem. We direct the reader to the supplementary materials for the exact definitions of these parameters. Here, it suffices to say that the inference problem must include the vector <span class="math inline">\(\underline{\bar{s}}_T\)</span> of all population mean fitness values and the matrix <span class="math inline">\(\underline{\underline{F}}\)</span> of all barcode frequencies within the sequencing data. With these nuisance variables in hand, the full inference problem we must solve takes the form <span id="eq-bayes_full"><span class="math display">\[
\pi(
    \underline{s}^M, \underline{\bar{s}}_T, \underline{\underline{F}} \mid
    \underline{\underline{R}}
) \propto
\pi(
    \underline{\underline{R}} \mid
    \underline{s}^M, \underline{\bar{s}}_T, \underline{\underline{F}}
)
\pi(
    \underline{s}^M, \underline{\bar{s}}_T, \underline{\underline{F}}
).
\tag{16}\]</span></span> To recover the marginal distribution over the non-neutral barcodes relative fitness values, we can numerically integrate out all nuisance parameters, i.e., <span id="eq-bayes_full_marginal"><span class="math display">\[
\pi(\underline{s}^M \mid \underline{\underline{R}}) =
\int d^{T-1}\underline{\bar{s}}_T
\int d^{B}\underline{f}_1 \cdots
\int d^{B}\underline{f}_T
\;
\pi(
    \underline{s}^M, \underline{\bar{s}}_T, \underline{\underline{F}} \mid
    \underline{\underline{R}}
).
\tag{17}\]</span></span></p>
<section id="seq-split_posterior" class="level3">
<h3 class="anchored" data-anchor-id="seq-split_posterior">Factorizing the posterior distribution</h3>
<p>The left-hand side of <a href="#eq-bayes_full">Equation&nbsp;16</a> is extremely difficult to work with. However, we can take advantage of the structure of our inference problem to rewrite it in a more manageable form. Specifically, the statistical dependencies of our observations and latent variables allow us to factorize the joint distribution into the product of multiple conditional distributions. To gain some intuition about this factorization, let us focus on the inference of the population mean fitness values <span class="math inline">\(\underline{\bar{s}}_T\)</span>. <a href="#eq-logfreq_neutral">Equation&nbsp;5</a> relates the value of the population mean fitness to the neutral lineage frequencies and nothing else. This suggests that when writing the posterior for these population mean fitness parameters, we should be able to condition it only on the neutral lineage frequency values, i.e., <span class="math inline">\(\pi(\underline{\bar{s}}_T \mid \underline{\underline{F}}^N)\)</span>. We point the reader to <a href="#sec-bayes_def">Section&nbsp;5.2</a> for the full mathematical details on this factorization. For our purpose here, it suffices to say we can rewrite the joint probability distribution as a product of conditional distributions of the form <span id="eq-split_posterior"><span class="math display">\[
\pi(
    \underline{s}^M, \underline{\bar{s}}_T, \underline{\underline{F}} \mid
    \underline{\underline{R}}
) =
\pi(
    \underline{s}^M \mid \underline{\bar{s}}_T, \underline{\underline{F}}^M
)
\pi(
    \underline{\bar{s}}_T \mid \underline{\underline{F}}^N
)
\pi(\underline{\underline{F}} \mid \underline{\underline{R}}).
\tag{18}\]</span></span> Written in this form, <a href="#eq-split_posterior">Equation&nbsp;18</a> captures the three sources of uncertainty listed in <a href="#sec-fitness_model">Section&nbsp;2.3</a> in each term. Starting from right to left, the first term on the right-hand side of <a href="#eq-split_posterior">Equation&nbsp;18</a> accounts for the uncertainty when inferring the frequency values given the barcode reads. The second term accounts for the uncertainty in the values of the mean population fitness at different time points. The last term accounts for the uncertainty in the parameter we care about—the mutants’ relative fitnesses. We refer the reader to <a href="#sec-bayes_def">Section&nbsp;5.2</a> for an extended description of the model with specific functional forms for each term on the left-hand side of <a href="#eq-split_posterior">Equation&nbsp;18</a> as well as the extension of the model to account for multiple experimental replicates or hierarchical genotypes.</p>
</section>
<section id="variational-inference" class="level3">
<h3 class="anchored" data-anchor-id="variational-inference">Variational Inference</h3>
<p>One of the technical challenges to the adoption of Bayesian methods is the analytical intractability of integrals such as that of <a href="#eq-bayes_full_marginal">Equation&nbsp;17</a>. Furthermore, even though efficient Markov Chain Monte Carlo (MCMC) algorithms such as Hamiltonian Montecarlo can numerically perform this integration <span class="citation" data-cites="betancourt2017"><a href="#ref-betancourt2017" role="doc-biblioref">[6]</a></span>, the dimensionality of the problem in <a href="#eq-split_posterior">Equation&nbsp;18</a> makes an MCMC-based approach prohibitively slow.</p>
<p>To overcome this computational limitation, we rely on the recent development of the automatic differentiation variational inference algorithm (ADVI) <span class="citation" data-cites="kucukelbir2016"><a href="#ref-kucukelbir2016" role="doc-biblioref">[7]</a></span>. Briefly, when performing ADVI, our target posterior distribution <span class="math inline">\(\pi(\theta \mid \underline{\underline{R}})\)</span>, where <span class="math inline">\(\theta = (\underline{s}^M, \underline{\bar{s}}_T, \underline{\underline{F}})\)</span>, is replaced by an approximate posterior distribution <span class="math inline">\(q_\phi(\theta)\)</span>, where <span class="math inline">\(\phi\)</span> fully parametrizes the approximate distribution. As further explained in <a href="#sec-vi_primer">Section&nbsp;5.1</a>, the numerical integration problem is replaced by an optimization problem of the form <span id="eq-vi_objective"><span class="math display">\[
q^*_\phi(\theta) = \min _\phi
D_{KL}(
    q_\phi(\theta) \lvert \lvert
    \pi(\theta \mid \underline{\underline{R}})
),
\tag{19}\]</span></span> where <span class="math inline">\(D_{KL}\)</span> is the Kulback-Leibler divergence. In other words, the complicated high-dimensional numerical integration problem is transformed into a much simpler problem of finding the value of the parameters <span class="math inline">\(\phi\)</span> such that <a href="#eq-vi_objective">Equation&nbsp;23</a> is satisfied as best as possible within some finite computation time. Although to compute <a href="#eq-vi_objective">Equation&nbsp;23</a>, we require the posterior distribution we are trying to approximate <span class="math inline">\(\pi(\theta \mid \underline{\underline{R}})\)</span>, it can be shown that maximizing the so-called evidence lower bound (ELBO) <span class="citation" data-cites="kingma2014"><a href="#ref-kingma2014" role="doc-biblioref">[9]</a></span>—equivalent to minimizing the variational free energy <span class="citation" data-cites="gottwald2020"><a href="#ref-gottwald2020" role="doc-biblioref">[10]</a></span>—is mathematically equivalent to performing the optimization prescribed by <a href="#eq-vi_objective">Equation&nbsp;23</a>. We direct the reader to <a href="#sec-vi_primer">Section&nbsp;5.1</a> for a short primer on variational inference.</p>
<p>This work is accompanied by the Julia library <code>BarBay.jl</code> that makes use of the implementation of both MCMC-based integration as well as ADVI optimization to numerically approximate the solution of <a href="#eq-bayes_full_marginal">Equation&nbsp;17</a> within the Julia ecosystem <span class="citation" data-cites="ge2018"><a href="#ref-ge2018" role="doc-biblioref">[11]</a></span>.</p>
<!-- Single dataset -->
</section>
</section>
<section id="inference-on-a-single-dataset" class="level2">
<h2 class="anchored" data-anchor-id="inference-on-a-single-dataset">Inference on a single dataset</h2>
<p>To assess the inference pipeline performance, we applied it to a simulated dataset with known ground truth relative fitness values (See <a href="#sec-logistic">Section&nbsp;5.4</a> for details on simulation). <a href="#fig-02">Figure&nbsp;2</a>(A) shows the structure of the synthetic dataset. The majority of barcodes of interest (faint color lines) are adaptive compared to the neutral barcodes (<span class="math inline">\(s^{(m)} &gt; 0\)</span>). Although the barcode frequency trajectories look relatively smooth, our fitness model requires the computation of the log frequency ratio between adjacent time points as derived in <a href="#eq-logfreq">Equation&nbsp;4</a>. <a href="#fig-02">Figure&nbsp;2</a>(B) shows such data transformation where we can better appreciate the observational noise input into our statistical model. This noise is evident for the darker lines representing the neutral barcodes since all of these lineages are assumed to be identically distributed.</p>
<p>To visualize the performance of our inference pipeline in fitting our fitness model to the observed data, we compute the so-called posterior predictive checks (PPC). In short, the PPC consists of repeatedly generating synthetic datasets in agreement with the results from the inference results. In other words, we use the resulting parameter values from the ADVI inference to generate possible datasets in agreement with the inferred values (See <a href="#sec-ppc">Section&nbsp;5.3</a> for further details on these computations). <a href="#fig-02">Figure&nbsp;2</a>(C) shows these results for all neutral lineages (upper left corner plot) and a few representative non-neutral barcodes. The different color shades represent the 95%, 68%, and 5% credible regions, i.e., the regions where we expect to find the data with the corresponding probability—or in terms of our parameter, the <span class="math inline">\(X\%\)</span> credible region is the interval where we expect the true parameter value to lie with <span class="math inline">\(X\%\)</span> probability.</p>
<p>The main advantage of our method is this natural interpretability of these credible regions where an <span class="math inline">\(X\%\)</span> credible region indeed captures the region of parameter space where we expect with <span class="math inline">\(X\%\)</span> probability the actual value of the parameter lies given our statistical model, our prior information, and the observed experimental data. A common mistake in the literature is interpreting frequentist confidence intervals as Bayesian credible regions when they are not equivalent <span class="citation" data-cites="morey2016"><a href="#ref-morey2016" role="doc-biblioref">[12]</a></span>. Frequentist confidence intervals and Bayesian credible regions are based on fundamentally different philosophical approaches to statistics. Frequentist confidence intervals represent the range of values that would contain the true population parameter with a certain probability if the experiment was repeated many times. The confidence interval does not represent the probability that the interval contains the true value. According to a specific model and prior information, Bayesian credible regions represent the range of values that contain the parameter with a certain posterior probability. The credible region directly represents the probability that the region contains the true value. So, frequentist confidence intervals cannot be interpreted as Bayesian credible regions because they have fundamentally different meanings. Treating an <span class="math inline">\(X\%\)</span> confidence interval like an <span class="math inline">\(X\%\)</span> credible region is fallacious since confidence intervals do not represent probabilistic coverage of the true value like credible regions. The intervals are generated through entirely different procedures.</p>
<p>To capture the global performance of the model, <a href="#fig-02">Figure&nbsp;2</a>(D) compares the known ground truth with the inferred relative fitness value for all barcodes of interest. There is an excellent degree of correspondence between these values, with the error bars representing the 68% credible region for the parameter value crossing the identity line for most barcodes. This latter point is made clear with <a href="#fig-02">Figure&nbsp;2</a>(E) where <span class="math inline">\(\approx 90\%\)</span> of ground truth fitness values fall within one standard deviation of the mean in the inferred posterior distributions.</p>
<div id="fig-02" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./figs/fig02.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;2: <strong>Single dataset inference</strong>. (A) Frequency trajectories that represent the raw data going into the inference. (B) Log frequency ratio between two adjacent time points used by the inference pipeline. Darker lines represent the neutral barcodes. These transformed data are much more noisy than the seemingly smooth frequency trajectories. (C) Examples of the posterior predictive checks for all neutral lineages (upper left panel) and a subset of representative mutant lineages. Shaded regions represent the 95%, 68%, and 5% credible regions for the data. The reported errors above the plot represent the 68% credible region on the mutant relative fitness marginal distribution. (D) Comparison between the ground truth fitness value from the logistic-growth simulation and the inferred fitness value. Gray error bars represent the 68% posterior credible region for the relative fitness values. (E) The empirical cumulative distribution function (ECDF) for the absolute z-score value of the ground truth parameter value within the inferred fitness posterior distribution.</figcaption>
</figure>
</div>
<!-- Multi-environment results -->
</section>
<section id="sec-multienv" class="level2">
<h2 class="anchored" data-anchor-id="sec-multienv">Fitness inference on multiple environments</h2>
<p>The fitness model in <a href="#eq-fitness">Equation&nbsp;3</a> relates nuisance parameters such as the population mean fitness and the barcode frequencies to the relative fitness parameter we want to infer from the data. These dependencies imply that uncertainty on the estimates of these nuisance parameters influences the inference of the relevant parameters. For example, imagine a scenario where the neutral lineages data were incredibly noisy, leading to poor estimates of the population mean fitness values <span class="math inline">\(\underline{\bar{s}}_T\)</span>. Since the relative fitness of any non-neutral barcode <span class="math inline">\(s^{(m)}\)</span> is determined with respect to these neutral barcodes, not accounting for the lack of precision in the value of the population mean fitness would result in misleading estimates of the accuracy with which we determine the value of the parameter we care about. Thus, propagating these sources of uncertainty in nuisance parameters is vital to generate an unbiased estimate of the relevant information we want to extract from the data. One of the benefits of Bayesian methods is the intrinsic error propagation embedded in the mathematical framework. For our previous example, the uncertainty on the value of the population mean fitness values is propagated to the relative fitness of a non-neutral barcode since we defined a joint posterior distribution over all parameters as fully expressed in <a href="#eq-bayes_full">Equation&nbsp;16</a>.</p>
<p>This natural error propagation can help us with the experimental design schematized in <a href="#fig-03">Figure&nbsp;3</a>(A). Here, rather than performing growth-dilution cycles in the same environment, the cells are diluted into a different environment. Thus, the uncertainty on the fitness estimate for the previous environment must be propagated to that of the next one. To validate the extension of our statistical model to this scenario, <a href="#fig-03">Figure&nbsp;3</a>(B) shows the trajectory of the log frequency ratios between adjacent time points. The different colored regions correspond to the different environments. For this simulation, the growth rate of Environment 2 was set to be, on average, half of the average growth rate in Environment 1. Equivalently, the growth rate in Environment 3 was set to be, on average, twice the average growth rate in Environment 1. <a href="#fig-03">Figure&nbsp;3</a>(C-E) show the correspondence between the simulation ground truth and the inferred fitness values, where the error bars represent the 68% credible region. <a href="#fig-03">Figure&nbsp;3</a>(F) summarizes the performance of our inference pipeline by showing the empirical cumulative distribution functions for the absolute value of the ground truth fitness value z-score within the posterior distribution. This plot shows that, overall, <span class="math inline">\(\approx 75\%\)</span> of inferred mean values fall within one standard deviation of the ground truth. For completeness, <a href="#fig-03">Figure&nbsp;3</a>(G) shows the posterior predictive checks for a few example barcodes.</p>
<div id="fig-03" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./figs/fig03.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;3: <strong>Multi-environment fitness inference.</strong> (A) Schematic of the simulated experimental design where growth-dilution cycles are performed into different environments for each cycle. (B) log frequency rations between adjacent time points. Darker lines represent the neutral barcodes. The colors in the background demark the corresponding environment, matching colors in (A). Environment 2 is set to have, on average, half the growth rate of environment 1. Likewise, environment 3 is set to have, on average, twice the growth rate of environment 1. (C-E) Comparison between the ground truth fitness value from the logistic-growth simulation and the inferred fitness value for each environment. Gray error bars represent the 68% posterior credible region. (F) The empirical cumulative distribution function (ECDF) for the absolute z-score value of the ground truth parameter value within the inferred fitness posterior distribution for all fitness values (black line) and each environment individually (color lines). (G) Examples of the posterior predictive checks for all neutral lineages (upper left panel) and a subset of representative mutant lineages. Shaded regions surrounding the data represent the 95%, 68%, and 5% credible regions for the data. The reported errors above the plot represent the 68% credible region on the mutant relative fitness marginal distribution. Background colors match those of (A).</figcaption>
</figure>
</div>
<!-- Hierarchical model for experimental replicates -->
</section>
<section id="sec-replicates" class="level2">
<h2 class="anchored" data-anchor-id="sec-replicates">Accounting for experimental replicates via hierarchical models</h2>
<p>Our inference pipeline can be extended to account for multiple experimental replicates via Bayesian hierarchical models <span class="citation" data-cites="betancourt2013"><a href="#ref-betancourt2013" role="doc-biblioref">[13]</a></span>. Briefly, when accounting for multiple repeated measurements of the same phenomena, there are two extreme cases one can use to perform the data analysis: On the one hand, we can treat each measurement as entirely independent, losing the power to utilize multiple measurements when trying to learn a single parameter. This can negatively impact the inference since, in principle, the value of our parameter of interest should not depend on the particular experimental replicate in question. However, this approach does not allow us to properly “combine” the uncertainties in both experiments when performing the inference. On the other hand, we can pool all data together and treat our different experiments as a single measurement with higher coverage. This loses the subtle differences due to biotic and abiotic batch effects, effectively halving the data that goes into our inference problem.</p>
<p>Hierarchical models present a middle ground between these extremes. First, hierarchical models rely on the definition of so-called <em>hyper-parameters</em>, that capture the parametric inference we are interested in—for this inference problem, we have a hyper-fitness value <span class="math inline">\(\theta^{(m)}\)</span> for each non-neutral barcode. Second, each experiment draws randomly from the distribution of this hyper-parameter, allowing for subtle variability between experiments to be accounted for—in the present inference pipeline, each experimental replicate gets assigned a <em>local</em> fitness value <span class="math inline">\(s^{(m)}_i\)</span>, where the extra sub-index indicates the <span class="math inline">\(i\)</span>-th experimental replicate. Conceptually, we can think of the local fitness for replicate <span class="math inline">\(i\)</span> as being sampled from a distribution that depends on the value of the global hyper-fitness value, i.e., <span class="math inline">\(s^{(m)}_i \sim \pi_{\theta^{(m)}}\)</span>, where the subindex <span class="math inline">\(\theta^{(m)}\)</span> indicates the distribution’s parametric dependence on the hyper-fitness value. This way of interpreting the connection between the distribution <span class="math inline">\(\pi_{\theta^{(m)}}\)</span> and the local fitness implies that a large replicate-to-replicate variability would lead to a broad hyper-fitness distribution—implying a large uncertainty when determining the parameter that characterizes the overall relative fitness. We point the reader to <a href="#sec-hierarchical_model">Section&nbsp;5.2.4</a> for the full definition of the hierarchical model used in this section. Importantly, as schematized in <a href="#fig-04">Figure&nbsp;4</a>(A), the influence between different experimental replicates runs both ways. First, the data from one experimental replicate (<span class="math inline">\(\underline{\underline{R}}^M_k\)</span> in the diagram) informs all local fitness values via the global hyper-fitness (upper panel in <a href="#fig-04">Figure&nbsp;4</a>(A)). Second, the local fitness value is informed by the data from all experimental replicates via the same global hyper-fitness parameter (lower panel in <a href="#fig-04">Figure&nbsp;4</a>(A)).</p>
<p>To test the performance of this model, we simulated two experimental replicates with 1000 unique barcodes (see <a href="#fig-04">Figure&nbsp;4</a>(B-C)) where we randomly sampled a ground truth hyper-fitness value <span class="math inline">\(\theta^{(m)}\)</span> for each barcode. We sampled a variation from this hyper-fitness value for each experimental replicate <span class="math inline">\(s^{(m)}_i\)</span> to capture experimental batch effects. <a href="#fig-04">Figure&nbsp;4</a>(D) shows the relationship between hyper-fitness and replicate fitness values for this simulation. The spread around the identity line represents the expected batch-to-batch variation. The posterior predictive checks examples in <a href="#fig-04">Figure&nbsp;4</a>(E) show that the hierarchical model can correctly fit the data for each experimental replicate. Furthermore, <a href="#fig-04">Figure&nbsp;4</a>(F-G) show a high correlation between the ground truth and the inferred fitness values. The empirical cumulative distribution functions shown in <a href="#fig-04">Figure&nbsp;4</a>(H-I) reveal that for <span class="math inline">\(\approx 75\%\)</span> of the non-neutral barcodes, the ground truth hyper-fitness values fall within one standard deviation from the mean value in the posterior distributions.</p>
<div id="fig-04" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./figs/fig04.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;4: <strong>Hierarchical model on experimental replicates</strong>. (A) Schematic depiction of the interactions between local fitness values <span class="math inline">\(\underline{s}_k^M\)</span> through the global hyper-fitness value <span class="math inline">\(\underline{\theta}^M\)</span> for <span class="math inline">\(K\)</span> hypothetical experimental replicates. The upper diagram shows how the data from replicate <span class="math inline">\(k\)</span> informs all local fitness values via the hyper-fitness parameter. The lower panel shows the reverse, where all other datasets inform the local fitness value. (B-C) Simulated replicate datasets with 900 barcodes of interest and 100 neutral lineages. (D) Comparison between the simulation ground truth hyper-fitness and each replicate ground truth fitness. The scatter between parameters captures experimental batch effects. (E) Examples of the posterior predictive checks for all neutral lineages (upper left panels) and a subset of representative mutant lineages. Shaded regions from light to dark represent the 95%, 68%, and 5% credible regions. (F-G) Comparison between the simulation’s ground truth hyper-fitness (F) and replicate fitness (G) values and the inferred parameters. Gray error bars represent the 68% posterior credible region. (H-I) The empirical cumulative distribution function (ECDF) for the absolute z-score value of the ground truth parameter value within the inferred hyper-fitness posterior distribution (H) and replicate fitness (I).</figcaption>
</figure>
</div>
<p>As shown in <a href="#fig-05">Figure&nbsp;5</a>, the structure imposed by the hierarchical model schematized in <a href="#fig-04">Figure&nbsp;4</a>(A), where we explicitly account for the connection between experimental replicates can improve the quality of the inference. Inferred fitness values between experimental replicates exhibit a stronger degree of correlation in the hierarchical model (<a href="#fig-05">Figure&nbsp;5</a>(A)) compared to conducting inference on each replicate independently (<a href="#fig-05">Figure&nbsp;5</a>(B)). Moreover, when comparing the inferred hyper-fitness values—the objective parameter when performing multiple experimental measurements—the hierarchical model outperforms averaging the independent experimental replicates as shown in <a href="#fig-05">Figure&nbsp;5</a>(C) and (D).</p>
<div id="fig-05" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./figs/fig05.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;5: <strong>Comparison between hierarchical model and single dataset model</strong>. (A-B) comparison of inferred fitness values between experimental replicates when fitting a hierarchical model (A) or independently fitting each dataset (B). Gray error bars represent the 68% credible regions. (C) Comparison between the ground truth hyper-fitness value and the inferred parameters. The blue dots show the inferred hyper-fitness values when assuming a hierarchical model. Gray error bars show the 68% credible region for this inference. The yellow dots show the average of the mean inferred fitness values for the two experimental replicates. No error bars are shown for these, as it is inappropriate to compute one with two data points per non-neutral barcode. (D) Empirical cumulative distribution function (ECDF) of the absolute difference between the inferred mean and the ground truth hyper-fitness.</figcaption>
</figure>
</div>
<!-- Hierarchical model for genotypes-->
</section>
<section id="sec-genotypes" class="level2">
<h2 class="anchored" data-anchor-id="sec-genotypes">Accounting for multiple barcodes per genotype via hierarchical models</h2>
<p>Hierarchical models can also capture another experimental design in which multiple barcodes map to the same or an equivalent genotype. As we will show, this many-to-one mapping can improve the inference compared to the extreme cases of inferring the fitness of each barcode independently or pooling the data of all barcodes mapping to a single genotype. As schematized in <a href="#fig-06">Figure&nbsp;6</a>(A), a small modification of the base model allows us to map the structure of our original model to that of a hierarchical model with a fitness hyperparameter vector <span class="math inline">\(\underline{\theta}^G\)</span>, where <span class="math inline">\(G\)</span> is the number of genotypes in the dataset.</p>
<p><a href="#fig-06">Figure&nbsp;6</a>(B) shows a single experimental replicate in which 90 genotypes were assigned a random number of barcodes (a multinomial distribution with a mean of ten barcodes per genotype) for a total of 900 non-neutral barcodes. To assess the performance of the hierarchical model proposed in <a href="#fig-06">Figure&nbsp;6</a>(A), we performed inference using this hierarchical model, as well as the two extreme cases of ignoring the connection between the barcodes belonging to the same genotype—equivalent to performing inference using the model presented in <a href="#fig-02">Figure&nbsp;2</a>(A) over the barcodes—or pooling the data of all barcodes belonging to the same genotype into a single count—equivalent to performing inference using the model presented in <a href="#fig-02">Figure&nbsp;2</a>(A) over the pooled barcodes. <a href="#fig-06">Figure&nbsp;6</a>(C-D) shows the comparison between the simulation ground truth and the inferred values for these three cases. Not only do the hierarchical model results show higher degrees of correlation with the ground truth, but the error bars (representing the 68% credible regions) are smaller, meaning that the uncertainty in the estimate of the parameter we care about decreases when using the hierarchical model. The improvement in the prediction can be seen in <a href="#fig-06">Figure&nbsp;6</a>(F) where the empirical cumulative distribution function of the absolute difference between the mean inferred value and the simulation ground truth is shown for all three inference models. The hierarchical model’s curve ascends more rapidly, showing that, in general, the inferred values are closer to the ground truth. For completeness, <a href="#fig-06">Figure&nbsp;6</a>(G) shows some examples of how the hierarchical model can capture the raw log-frequency count observations.</p>
<div id="fig-06" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./figs/fig06.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;6: <strong>Hierarchical model for multiple barcodes per genotype.</strong> (A) Schematic depiction of the hierarchical structure for multiple barcodes mapping to a single genotype. A set of barcodes mapping to an equivalent genotype map to “local” fitness values <span class="math inline">\(s^{(b)}\)</span> that are connected via a hyper-fitness parameter for the genotype <span class="math inline">\(\theta^{(g)}\)</span>. (B) Simulated dataset with 100 neutral lineages and 900 barcodes of interest distributed among 90 genotypes. (C-E) Comparison between the inferred and ground truth fitness values for a hierarchical model (C), a model where each barcode is inferred independently (D), and a model where barcodes mapping to the same genotype are pooled together (E). Gray error bars represent the 68% credible regions. (F) Empirical cumulative distribution function (ECDF) of the absolute difference between the inferred mean and the ground truth fitness values for all three models. (G) Examples of the posterior predictive checks for all neutral lineages (upper left panels) and a subset of representative mutant lineages. Shaded regions from light to dark represent the 95%, 68%, and 5% credible regions.</figcaption>
</figure>
</div>
</section>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
<p>Experimental evolution of microbial systems has dramatically advanced our understanding of the basic principles of biological evolution <span class="citation" data-cites="kussell2013"><a href="#ref-kussell2013" role="doc-biblioref">[14]</a></span>. From questions related to the optimal fine-tuning of gene expression programs <span class="citation" data-cites="Dekel2005"><a href="#ref-Dekel2005" role="doc-biblioref">[15]</a></span>, to the dimensionality, geometry, and accessibility of the adaptive fitness landscape explored by these rapidly adapting populations <span class="citation" data-cites="kinsler2020 maeda2020"><a href="#ref-kinsler2020" role="doc-biblioref">[4]</a>, <a href="#ref-maeda2020" role="doc-biblioref">[16]</a></span>, to the emergence of eco-evolutionary dynamics in a long-term evolution experiment <span class="citation" data-cites="good2017"><a href="#ref-good2017" role="doc-biblioref">[17]</a></span>; for all of these and other cases, the microbial experimental platform combined with high-throughput sequencing has been essential to tackling these questions with empirical data. This exciting research area promises to improve as new culturing technologies <span class="citation" data-cites="jagdish2022"><a href="#ref-jagdish2022" role="doc-biblioref">[18]</a></span> as well as more complex lineage barcoding schemes <span class="citation" data-cites="nguyenba2019a yang2022"><a href="#ref-nguyenba2019a" role="doc-biblioref">[2]</a>, <a href="#ref-yang2022" role="doc-biblioref">[19]</a></span>, are adopted.</p>
<p>For this data-heavy field, properly accounting for the uncertainty in parameters inferred from experiments is vital to ensure the conclusions drawn are reliable. Bayesian statistics presents a principled way to quantify this uncertainty systematically <span class="citation" data-cites="gelman2010"><a href="#ref-gelman2010" role="doc-biblioref">[20]</a></span>. Moreover, Bayesian analysis offers a more natural way to interpret the role that probability theory plays when performing data analysis compared to the often-misinterpreted frequentist methods <span class="citation" data-cites="vanderplas2014"><a href="#ref-vanderplas2014" role="doc-biblioref">[21]</a></span>. Nevertheless, the technical challenges associated with Bayesian analysis has limited its application. This is set to change as recognition of the misuse of frequentist concepts such as the p-value is receiving more attention <span class="citation" data-cites="nuzzo2014"><a href="#ref-nuzzo2014" role="doc-biblioref">[22]</a></span>. Moreover, advances in numerical methods such as Hamiltonian Monte Carlo <span class="citation" data-cites="betancourt2017"><a href="#ref-betancourt2017" role="doc-biblioref">[6]</a></span> and variational inference <span class="citation" data-cites="kucukelbir2016"><a href="#ref-kucukelbir2016" role="doc-biblioref">[7]</a></span> allows for complex Bayesian models to be fit to empirical data.</p>
<p>In this paper, we present a computational pipeline to analyze lineage-tracking time-series data for massive-parallel competition assays. More specifically, we fit a Bayesian model to infer the fitness of multiple genotypes relative to a reference <span class="citation" data-cites="kinsler2020 ascensao2023"><a href="#ref-ascensao2023" role="doc-biblioref">[3]</a>, <a href="#ref-kinsler2020" role="doc-biblioref">[4]</a></span>. The proposed model accounts for multiple sources of uncertainty with proper error propagation intrinsic to Bayesian methods. To scale the inference pipeline to large datasets with <span class="math inline">\(&gt; 10,000\)</span> barcodes, we use the ADVI algorithm <span class="citation" data-cites="kucukelbir2016"><a href="#ref-kucukelbir2016" role="doc-biblioref">[7]</a></span> to fit a variational posterior distribution. The main difference between our method and previous inference pipelines, such as <span class="citation" data-cites="li2023"><a href="#ref-li2023" role="doc-biblioref">[23]</a></span>, is that the present analysis provides interpretable errors on the inferred fitness values. The reported uncertainty intervals—known as credible regions—can be formally interpreted as capturing the corresponding probability mass of finding the true value of the parameter given the model, the prior information, and the data. Furthermore, minor modifications to the structure of the statistical model presented in this work allow for the analysis of different experimental designs, such as growth-dilution cycles in different environments, joint analysis of multiple experimental replicates of the same experiment via hierarchical models, and a hierarchical model for multiple barcodes mapping to equivalent genotypes. We validate our analysis pipeline on simulated datasets with known ground truth, showing that the model fits the data adequately, capturing the ground truth parameters within the posterior distribution.</p>
<p>It is important to highlight some of the consequences of the general experimental design and the implicit assumptions within the proposed statistical model to analyze the resulting data. First, the composition of the population is such that the initial fraction of the population occupied by the barcoded genotypes is small—usually &gt;90% of the initial population is the non-labeled reference strain. This constraint is important as the fitness model used to fit the time series data assumes that the tracked frequencies are <span class="math inline">\(\ll 1\)</span>. Second, when computing log frequency ratios, we can run into the issue of dividing by zero. This is a common problem when dealing with molecular count data <span class="citation" data-cites="lovell2020"><a href="#ref-lovell2020" role="doc-biblioref">[24]</a></span>. Our model gets around this issue by assuming that the frequency of any barcode cannot be, but still can get arbitrarily close to, zero. Therefore, we implicitly assume that no lineage goes extinct during the experiment. Moreover, the statistical model directly accounts for the uncertainty associated with having zero barcode counts, increasing the corresponding uncertainty. Third, the models presented in this paper require the existence of a labeled sub-population of barcoded reference strains. These barcodes help determine the fitness baseline, as every fitness is quantified with respect to this reference genotype. This experimental design constraint facilitates the inference of the population mean fitness since most of the culture—the unlabeled reference genotype—is not tracked. Finally, the presented statistical model assumes that relative fitness is solely a constant of the environment and the genotype. Future directions of this work could extend the fitness model to properly analyze data with time-varying or frequency-dependent fitness values.</p>
<p>In total, the statistical model presented in this work and the software package accompanying the paper allow for a principled way of quantifying the accuracy with which we can extract relevant parametric information from large-scale multiplexed fitness competition assays. Furthermore, the implementation of Bayesian models and their fitting via automatic differentiation approaches opens the gate to extend this type of formal analysis to the data-rich literature in experimental evolution and other high-throughput technologies applications.</p>
</section>
<section id="acknowledgements" class="level1">
<h1>Acknowledgements</h1>
<p>We would like to thank Griffin Chure and Michael Betancourt for their helpful advice and discussion. We would like to thank Karna Gowda, Spencer Farrell, and Shaili Mathur for critical observations on the manuscript. This work was supported by the NIH/NIGMS, Genomics of rapid adaptation in the lab and in the wild R35GM11816506 (MIRA grant), the NIH, Unravelling mechanisms of tumor suppression in lung cancer R01CA23434903, the NIH (PQ4), Quantitative and multiplexed analysis of gene function in cancer in vivo R01CA23125303, the NIH, Genetic Determinants of Tumor Growth and Drug Sensitivity in EGFR Mutant Lung Cancer R01CA263715, the NIH, Dissecting the interplay between aging, genotype, and the microenvironment in lung cancer U01AG077922, the NIH, Genetic dissection of oncogenic Kras signaling R01CA230025, and the CZ Biohub investigator program. MRM was supported by the Schmidt Science Fellowship. MM was supported by The National Science Foundation-Simons Center for Quantitative Biology at Northwestern University and the Simons Foundation grant 597491. MM is a Simons Investigator. DP is a CZ Biohub investigator.</p>
</section>
<section id="supplementary-materials" class="level1">
<h1>Supplementary Materials</h1>
<!-- Primer on Variational Bayes -->
<section id="sec-vi_primer" class="level2">
<h2 class="anchored" data-anchor-id="sec-vi_primer">Primer on Variational Inference</h2>
<p>In this section, we will briefly introduce the idea behind variational inference. Recall that any Bayesian inference problem deals with the joint distribution between observations <span class="math inline">\(\underline{x}\)</span> and unobserved latent variables <span class="math inline">\(\underline{\theta}\)</span>. This joint distribution can be written as the product of a distribution of the observations <span class="math inline">\(\underline{x}\)</span> conditioned on the <span class="math inline">\(\underline{\theta}\)</span> and the marginal distribution of these latent variables, i.e., <span id="eq-vi_joint_dist"><span class="math display">\[
\pi(\underline{x}, \underline{\theta}) =
\pi(\underline{x} \mid \underline{\theta}) \pi(\underline{\theta}).
\tag{20}\]</span></span> A Bayesian inference pipeline’s objective is to compute the latent variables’ posterior probability given a set of observations. This computation is equivalent to updating our prior beliefs about the set of values that the latent variables take after taking in new data. We write this as Bayes theorem <span id="eq-vi_bayes_theorem"><span class="math display">\[
\pi(\underline{\theta} \mid \underline{x}) =
\frac{
        \pi(\underline{x} \mid \underline{\theta})\pi(\underline{\theta})
    }{
        \pi(\underline{x})
    }.
\tag{21}\]</span></span> The main technical challenge for working with <a href="#eq-vi_bayes_theorem">Equation&nbsp;21</a> comes from the computation of the denominator, also known as the <em>evidence</em> or the <em>marginalized likelihood</em>. The reason computing this term is challenging is because it involves a (potentially) high-dimensional integral of the form <span id="eq-vi_evidence_int"><span class="math display">\[
\pi(\underline{x}) =
\int\cdots\int d^K\underline{\theta}\; \pi(\underline{x}, \underline{\theta}) =
\int\cdots\int d^K\underline{\theta}\;
\pi(\underline{x} \mid \underline{\theta})
\pi(\underline{\theta}),
\tag{22}\]</span></span> where <span class="math inline">\(K\)</span> is the dimesionality of the <span class="math inline">\(\underline{\theta}\)</span> vector. Here, the integrals are taken over the support—the set of values valid for the distribution—of <span class="math inline">\(\pi(\underline{\theta})\)</span>. However, only a few selected distributions have a closed analytical form; thus, in most cases <a href="#eq-vi_evidence_int">Equation&nbsp;22</a> must be solved numerically.</p>
<p>Integration in high-dimensional spaces can be computationally extremely challenging. For a naive numerical quadrature procedure, integrating over a grid of values for each dimension of <span class="math inline">\(\underline{\theta}\)</span> comes with an exponential explosion of the number of required grid point evaluations, most of which do not contribute significantly to the integration. To gain visual intuition about this challenge, imagine integrating the function depicted in <a href="#fig-SI01">Figure&nbsp;7</a>. If the location of the high-density region (dark peak) is unknown, numerical quadrature requires many grid points to ensure we capture this peak. However, most of the numerical evaluations of the function on the grid points do not contribute significantly to the integral. Therefore, our computational resources are wasted on insignificant evaluations. This only gets worse as the number of dimensions increases since the number of grid point evaluation scales exponentially.</p>
<div id="fig-SI01" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./figs/figSIX_gaussian_peak.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;7: <strong>High-dimensional numerical quadrature does not scale with dimensionality.</strong> Schematic depiction of the problem with naive numerical quadrature to integrate over an unknown density. While the density is concentrated on the dark peak, most of the evaluations over the <span class="math inline">\(x_1-x_2\)</span> grid do not contribute to the value of the integral</figcaption>
</figure>
</div>
<p>Modern Markov Chain Monte Carlo algorithms, such as Hamiltonian Monte Carlo, can efficiently perform this high-dimensional integration by utilizing gradient information from the target density <span class="citation" data-cites="betancourt2017"><a href="#ref-betancourt2017" role="doc-biblioref">[6]</a></span>. Nevertheless, these sampling-based methods become prohibitively slow for the number of dimensions our present inference problem presents. Thus, there is a need to find scalable methods for the inference problem in <a href="#eq-vi_bayes_theorem">Equation&nbsp;21</a>.</p>
<p>Variational inference circumvents these technical challenges by proposing an approximate solution to the problem. Instead of working with the posterior distribution in its full glory <span class="math inline">\(\pi(\underline{\theta} \mid \underline{x})\)</span>, let us propose an approximate posterior distribution <span class="math inline">\(q_\phi\)</span> that belongs to a distribution family fully parametrized by <span class="math inline">\(\phi\)</span>. For example, let us say that the distribution <span class="math inline">\(q_\phi\)</span> belongs to the family of multivariate Normal distributions such that <span class="math inline">\(\phi = (\underline{\mu}, \underline{\underline{\Sigma}})\)</span>, where <span class="math inline">\(\underline{\mu}\)</span> is the vector of means and <span class="math inline">\(\underline{\underline{\Sigma}}\)</span> is the covariance matrix. If we replace <span class="math inline">\(\pi\)</span> by <span class="math inline">\(q_\phi\)</span>, we want <span class="math inline">\(q_\phi\)</span> to resemble the original posterior as much as possible. Mathematically, this can be expressed as minimizing a “<em>distance metric</em>”—the Kullback-Leibler (KL) divergence, for example—between the distributions. Note that we use quotation marks because, formally, the KL divergence is not a distance metric since it is not symmetric. Nevertheless, the variational objective is set to find a distribution <span class="math inline">\(q_\phi^*\)</span> such that <span id="eq-vi_objective"><span class="math display">\[
q_\phi^*(\underline{\theta}) =
\min_\phi D_{KL}\left(
    q_\phi(\underline{\theta}) \vert\vert
    \pi(\underline{\theta} \mid \underline{x})
\right),
\tag{23}\]</span></span> where <span class="math inline">\(D_{KL}\)</span> is the KL divergence. Furthermore, we highlight that the KL divergence is a strictly positive number, i.e., <span id="eq-vi_kl_pos"><span class="math display">\[
D_{KL}\left(
    q_\phi(\underline{\theta}) \vert\vert
    \pi(\underline{\theta} \mid \underline{x})
\right) \geq 0,
\tag{24}\]</span></span> as this property will become important later on.</p>
<p>At first sight, <a href="#eq-vi_objective">Equation&nbsp;23</a> does not improve the situation but only introduces further technical complications. After all, the definition of the KL divergence <span id="eq-vi_kl_div"><span class="math display">\[
D_{KL}\left(
    q_\phi(\underline{\theta}) \vert\vert
    \pi(\underline{\theta} \mid \underline{x})
\right) \equiv
\int \cdots \int d^K\underline{\theta}\;
q_\phi(\underline{\theta})
\ln \frac{
    q_\phi(\underline{\theta})
}{
    \pi(\underline{\theta} \mid \underline{x})
},
\tag{25}\]</span></span> includes the posterior distribution <span class="math inline">\(\pi(\underline{\theta} \mid \underline{x})\)</span> we are trying to get around. However, let us manipulate <a href="#eq-vi_kl_div">Equation&nbsp;25</a> to beat it to a more reasonable form. First, we can use the properties of the logarithms to write <span id="eq-vi_step01"><span class="math display">\[
D_{KL}\left(
    q_\phi(\underline{\theta}) \vert\vert
    \pi(\underline{\theta} \mid \underline{x})
\right) =
\int d^K\underline{\theta}\; q_\phi(\underline{\theta})
\ln q_\phi(\underline{\theta}) -
\int d^K\underline{\theta}\; q_\phi(\underline{\theta})
\ln \pi(\underline{\theta} \mid \underline{x}),
\tag{26}\]</span></span> where, for convenience, we write a single integration sign (<span class="math inline">\(d^K\underline{\theta}\;\)</span> still represents a multi-dimensional differential). For the second term in <a href="#eq-vi_step01">Equation&nbsp;26</a>, we can substitute the term inside the logarithm using <a href="#eq-vi_bayes_theorem">Equation&nbsp;21</a>. This results in <span id="eq-vi_step02"><span class="math display">\[
\begin{aligned}
D_{KL}\left(
    q_\phi(\underline{\theta}) \vert\vert
    \pi(\underline{\theta} \mid \underline{x})
\right) &amp;=
\int d^K\underline{\theta}\; q_\phi(\underline{\theta})
\ln q_\phi(\underline{\theta}) \\
&amp;- \int d^K\underline{\theta}\; q_\phi(\underline{\theta})
\ln \left(
    \frac{
        \pi(\underline{x} \mid \underline{\theta})\pi(\underline{\theta})
    }{
        \pi(\underline{x})
    }
\right).
\end{aligned}
\tag{27}\]</span></span> Again, using the properties of logarithms, we can split <a href="#eq-vi_step02">Equation&nbsp;27</a>, obtaining <span id="eq-vi_step03"><span class="math display">\[
\begin{aligned}
D_{KL}\left(
    q_\phi(\underline{\theta}) \vert\vert
    \pi(\underline{\theta} \mid \underline{x})
\right) &amp;=
\int d^K\underline{\theta}\; q_\phi(\underline{\theta})
\ln q_\phi(\underline{\theta}) \\
&amp;-\int d^K\underline{\theta}\; q_\phi(\underline{\theta})
\ln \pi(\underline{x} \mid \underline{\theta}) \\
&amp;-\int d^K\underline{\theta}\; q_\phi(\underline{\theta})
\ln \pi(\underline{\theta}) \\
&amp;+\int d^K\underline{\theta}\; q_\phi(\underline{\theta})
\ln \pi(\underline{x}).
\end{aligned}
\tag{28}\]</span></span> It is convenient to write <a href="#eq-vi_step03">Equation&nbsp;28</a> as <span id="eq-vi_step04"><span class="math display">\[
\begin{aligned}
D_{KL}\left(
    q_\phi(\underline{\theta}) \vert\vert
    \pi(\underline{\theta} \mid \underline{x})
\right) &amp;=
\int d^K\underline{\theta}\; q_\phi(\underline{\theta})
\ln \frac{
    q_\phi(\underline{\theta})
    }{
        \pi(\underline{\theta})
    } \\
&amp;-\int d^K\underline{\theta}\; q_\phi(\underline{\theta})
\ln \pi(\underline{x} \mid \underline{\theta}) \\
&amp;+ \ln \pi(\underline{x})
\int d^K\underline{\theta}\; q_\phi(\underline{\theta}),
\end{aligned}
\tag{29}\]</span></span> where for the last term, we can take <span class="math inline">\(\ln \pi(\underline{x})\)</span> out of the integral since it does not depend on <span class="math inline">\(\underline{\theta}\)</span>. Lastly, we utilize two properties:</p>
<ol type="1">
<li>The proposed approximate distribution must be normalized, i.e., <span id="eq-vi_q_norm"><span class="math display">\[
\int d^K\underline{\theta}\; q_\phi(\underline{\theta}) = 1.
\tag{30}\]</span></span></li>
<li>The law of the unconscious statistician (LOTUS) establishes that for any probability density function, it must be true that <span id="eq-vi_lotus"><span class="math display">\[
\int d^K\underline{\theta}\; q_\phi(\underline{\theta})
f(\underline{\theta}) = \left\langle
f(\underline{\theta})
\right\rangle_{q_\phi},
\tag{31}\]</span></span> where <span class="math inline">\(\left\langle\cdot\right\rangle_{q_\phi}\)</span> is the expected value over the <span class="math inline">\(q_\phi\)</span> distribution.</li>
</ol>
<p>Using these two properties, the positivity constraint on the KL divergence in <a href="#eq-vi_kl_pos">Equation&nbsp;24</a>, and the definition of the KL divergence in <a href="#eq-vi_kl_div">Equation&nbsp;25</a> we can rewrite <a href="#eq-vi_step04">Equation&nbsp;29</a> as <span id="eq-vi_step05"><span class="math display">\[
D_{KL}\left(
    q_\phi(\underline{\theta}) \vert \vert
    \pi(\underline{\theta})
\right) -
\left\langle
    \ln \pi(\underline{x} \mid \underline{\theta})
\right\rangle_{q_\phi}
\geq - \ln \pi(\underline{x}).
\tag{32}\]</span></span> Multiplying by a minus one, we have the functional form of the so-called evidence lower bound (ELBO) <span class="citation" data-cites="kingma2014"><a href="#ref-kingma2014" role="doc-biblioref">[9]</a></span>, <span id="eq-vi_elbo"><span class="math display">\[
\underbrace{
    \ln \pi(\underline{x})
}_{\text{log evidence}} \geq
\underbrace{
    \left\langle
        \ln \pi(\underline{x} \mid \underline{\theta})
    \right\rangle_{q_\phi} -
    D_{KL}\left(
        q_\phi(\underline{\theta}) \vert \vert
        \pi(\underline{\theta})
    \right)
}_{\text{ELBO}}.
\tag{33}\]</span></span></p>
<p>Let us recapitulate where we are. We started by presenting the challenge of working with Bayes’ theorem, as it requires a high-dimensional integral of the form in <a href="#eq-vi_evidence_int">Equation&nbsp;22</a>. As an alternative, variational inference posits to approximate the posterior distribution <span class="math inline">\(\pi(\underline{\theta} \mid \underline{x})\)</span> with a parametric distribution <span class="math inline">\(q_\phi(\underline{\theta})\)</span>. By minimizing the KL divergence between these distributions, we arrive at the result in <a href="#eq-vi_elbo">Equation&nbsp;33</a>, where the left-hand side—the log marginalized likelihood or log evidence—we cannot compute for technical/computational reasons. However, the right-hand side is composed of things we can easily evaluate. We can easily evaluate the log-likelihood <span class="math inline">\(\ln \pi(\underline{x} \mid \underline{\theta})\)</span> and the KL divergence between our proposed approximate distribution <span class="math inline">\(q_\phi(\underline{\theta})\)</span> and the prior distribution <span class="math inline">\(\pi(\underline{\theta})\)</span>. Moreover, we can compute the gradients of these functions with respect to the parameters of our proposed distribution. This last point implies that we can change the parameters of the proposed distribution to maximize the ELBO. And, although we cannot compute the left-hand side of <a href="#eq-vi_elbo">Equation&nbsp;33</a>, we know that however large we make the ELBO, it will always be smaller than (or equal) the log-marginal likelihood. Therefore, the larger we can make the ELBO by modifying the parameters <span class="math inline">\(\phi\)</span>, the closer it gets to the log-marginal likelihood, and, as a consequence, the better our proposed distribution <span class="math inline">\(q_\phi(\underline{\theta})\)</span> gets to the true posterior distribution <span class="math inline">\(\pi(\underline{\theta} \mid \underline{x})\)</span>.</p>
<p>In this sense, variational inference turns the intractable numerical integration problem to an optimization routine, for which there are several algorithms available.</p>
<section id="advi-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="advi-algorithm">ADVI algorithm</h3>
<p>To maximize the right-hand side of <a href="#eq-vi_elbo">Equation&nbsp;33</a>, the Automatic Differentiation Variational Inference (ADVI) algorithm developed in <span class="citation" data-cites="kucukelbir2016"><a href="#ref-kucukelbir2016" role="doc-biblioref">[7]</a></span> takes advantage of advances in probabilistic programming languages to generate a robust method to perform this optimization. Without going into the details of the algorithm implementation, for our purposes, it suffices to say that we define our joint distribution <span class="math inline">\(\pi(\underline{\theta}, \underline{x})\)</span> as the product defined in <a href="#eq-vi_joint_dist">Equation&nbsp;20</a>. ADVI then proposes an approximate variational distribution <span class="math inline">\(q_\phi\)</span> that can either be a multivariate Normal distribution with a diagonal covariance matrix, i.e., <span id="eq-vi_meanfield"><span class="math display">\[
\phi = (\underline{\mu}, \underline{\underline{D}}),
\tag{34}\]</span></span> where <span class="math inline">\(\underline{\underline{D}}\)</span> is the identity matrix, with the diagonal elements given by the vector of variances <span class="math inline">\(\underline{\sigma}^2\)</span> for each variable or a full-rank multivariate Normal distribution <span id="eq-vi_full"><span class="math display">\[
\phi = (\underline{\mu}, \underline{\underline{\Sigma}}).
\tag{35}\]</span></span></p>
<p>Then, the parameters are initialized in some value <span class="math inline">\(\phi_o\)</span>. These parameters are iteratively updated by computing the gradient of the ELBO (right-hand side of <a href="#eq-vi_elbo">Equation&nbsp;33</a>), hereafter defined as <span class="math inline">\(\mathcal{L}\)</span>, with respect to the parameters, <span id="eq-vi_elbo_gradient"><span class="math display">\[
\nabla_\phi \mathcal{L} = \nabla_{\underline{\mu}} \mathcal{L} +
\nabla_{\underline{\sigma}}\mathcal{L},
\tag{36}\]</span></span> and then computing <span class="math display">\[
\phi_{t+1} = \phi_{t} + \eta \nabla_\phi \mathcal{L},
\]</span> where <span class="math inline">\(\eta\)</span> defines the step size.</p>
<p>This short explanation behind the ADVI algorithm is intended only to gain intuition on how the optimal variational distribution <span class="math inline">\(q_\phi\)</span> be computed. There are many nuances in the implementation of the ADVI algorithm. We invite the reader to look at the original reference for further details.</p>
<!-- Bayesian inference pipeline -->
</section>
</section>
<section id="sec-bayes_def" class="level2">
<h2 class="anchored" data-anchor-id="sec-bayes_def">Defining the Bayesian model</h2>
<p>In the main text, we specify the inference problem we must solve as being of the form <span id="eq-full_inference_SI"><span class="math display">\[
\pi(
    \underline{s}^M, \underline{\bar{s}}_T, \underline{\underline{F}} \mid
    \underline{\underline{R}}
) \propto
\pi(
    \underline{\underline{R}} \mid
    \underline{s}^M, \underline{\bar{s}}_T, \underline{\underline{F}}
)
\pi(
    \underline{s}^M, \underline{\bar{s}}_T, \underline{\underline{F}}
).
\tag{37}\]</span></span> Here, we briefly define the missing nuisance parameters. Let <span id="eq-pop_fitness_vec"><span class="math display">\[
\underline{\bar{s}}_T = (\bar{s}_1, \bar{s}_2, \ldots, \bar{s}_{T-1})^\dagger,
\tag{38}\]</span></span> be the vector containing the <span class="math inline">\(T-1\)</span> population mean fitness we compute from the <span class="math inline">\(T\)</span> time points where measurements were taken. We have <span class="math inline">\(T-1\)</span> since the value of any <span class="math inline">\(\bar{s}_t\)</span> requires cycle numbers <span class="math inline">\(t\)</span> and <span class="math inline">\(t+1\)</span>. Furthermore, let the matrix <span class="math inline">\(\underline{\underline{F}}\)</span> be a <span class="math inline">\(T \times B\)</span> matrix containing all frequency values. As with <a href="#eq-R-mat_split">Equation&nbsp;12</a> in the main text, we can split <span class="math inline">\(\underline{\underline{F}}\)</span> into two matrices of the form <span id="eq-F-mat_split"><span class="math display">\[
\underline{\underline{F}} = \left[
\underline{\underline{F}}^N \; \underline{\underline{F}}^M
\right],
\tag{39}\]</span></span> to separate the corresponding neutral and non-neutral barcode frequencies.</p>
<p>Let us now define each of the terms in <a href="#eq-split_posterior">Equation&nbsp;18</a> described in <a href="#sec-bayesian_inference">Section&nbsp;2.4</a> of the main text. The following sections will specify the functional form each of these terms takes.</p>
<section id="sec-bayes_freq" class="level3">
<h3 class="anchored" data-anchor-id="sec-bayes_freq">Frequency uncertainty <span class="math inline">\(\pi(\underline{\underline{F}} \mid \underline{\underline{R}})\)</span></h3>
<p>We begin with the probability of the frequency values given the raw barcode reads. The first assumption is that the inference of the frequency values for time <span class="math inline">\(t\)</span> is independent of any other time. Therefore, we can write the joint probability distribution as a product of independent distributions of the form <span id="eq-freq_indep"><span class="math display">\[
\pi(\underline{\underline{F}} \mid \underline{\underline{R}}) =
\prod_{t=1}^T \pi(\underline{f}_t \mid \underline{r}_t),
\tag{40}\]</span></span> where <span class="math inline">\(\underline{f}_t\)</span> and <span class="math inline">\(\underline{r}_t\)</span> are the <span class="math inline">\(t\)</span>-th row of the matrix containing all of the measurements for time <span class="math inline">\(t\)</span>. We imagine that when the barcode reads are obtained via sequencing, the quantified number of reads is a Poisson sample from the “true” underlying number of barcodes within the pool. This translates to assuming that the number of reads for each barcode at any time point <span class="math inline">\(r^{(b)}_t\)</span> is an independent Poisson random variable, i.e., <span id="eq-freq_poisson_reads"><span class="math display">\[
r^{(b)}_t \sim \operatorname{Poiss}(\lambda^{(b)}_t),
\tag{41}\]</span></span> where the symbol “<span class="math inline">\(\sim\)</span>” is read “distributed as.” Furthermore, for a Poisson distribution, we have that <span id="eq-freq_poisson_lambda"><span class="math display">\[
\lambda^{(b)}_t = \left\langle r^{(b)}_t \right\rangle =
\left\langle
    \left( r^{(b)}_t - \left\langle r^{(b)}_t \right\rangle \right)^2
\right\rangle,
\tag{42}\]</span></span> where <span class="math inline">\(\left\langle \cdot \right\rangle\)</span> is the expected value. In other words the Poisson parameter is equal to the mean and variance of the distribution. The Poisson distribution has the convenient property that for two Poisson distributed random variables <span class="math inline">\(X \sim \operatorname{Poiss}(\lambda_x)\)</span> and <span class="math inline">\(Y \sim \operatorname{Poiss}(\lambda_y)\)</span>, we have that <span id="eq-freq_additivity"><span class="math display">\[
Z \equiv X + Y \sim \operatorname{Poiss}(\lambda_x + \lambda_y).
\tag{43}\]</span></span> This additivity allows us to write the total number of reads at time <span class="math inline">\(t\)</span> <span class="math inline">\(n_t\)</span> also as a Poisson-distributed random variable of the form <span id="eq-freq_poisson_total"><span class="math display">\[
n_t \sim \operatorname{Poiss}\left( \sum_{b=1}^B \lambda^{(b)}_t \right),
\tag{44}\]</span></span> where the sum is taken over all <span class="math inline">\(B\)</span> barcodes.</p>
<p>If the total number of reads is given by <a href="#eq-freq_poisson_total">Equation&nbsp;44</a>, the array with the number of reads for each barcode at time <span class="math inline">\(t\)</span>, <span class="math inline">\(\underline{r}_t\)</span> is then distributed as <span id="eq-freq_multinomial"><span class="math display">\[
\underline{r}_t \sim \operatorname{Multinomial}(n_t, \underline{f}_t),
\tag{45}\]</span></span> where each of the <span class="math inline">\(B\)</span> entries of the frequency vector <span class="math inline">\(\underline{f}_t\)</span> is a function of the <span class="math inline">\(\underline{\lambda}_t\)</span> vector, given by <span id="eq-freq_freq_lambda"><span class="math display">\[
f_t^{(b)} \equiv f_t^{(b)}(\underline{\lambda}_t) =
\frac{\lambda_t^{(b)}}{\sum_{b'=1}^B \lambda_t^{(b')}}.
\tag{46}\]</span></span> In other words, we can think of the <span class="math inline">\(B\)</span> barcode counts as independent Poisson samples or as a single multinomial draw with a random number of total draws, <span class="math inline">\(n_t\)</span>, and the frequency vector <span class="math inline">\(\underline{f}_t\)</span> we are interested in. Notice that <a href="#eq-freq_freq_lambda">Equation&nbsp;46</a> is a deterministic function that connects the Poisson parameters to the frequencies. Therefore, we have the equivalence that <span id="eq-freq_f_lambda_equiv"><span class="math display">\[
\pi(\underline{f}_t \mid \underline{r}_t) =
\pi(\underline{\lambda}_t \mid \underline{r}_t),
\tag{47}\]</span></span> meaning that the uncertainty comes from the <span class="math inline">\(\underline{\lambda}_t\)</span> vector. By Bayes theorem, we therefore write <span id="eq-freq_lambda_bayes"><span class="math display">\[
\pi(\underline{\lambda}_t \mid n_t, \underline{r}_t) \propto
\pi(n_t, \underline{r}_t \mid \underline{\lambda}_t) \pi(\underline{\lambda}_t),
\tag{48}\]</span></span> where we explicitly include the dependence on <span class="math inline">\(n_t\)</span>. This does not affect the distribution or brings more uncertainty because <span class="math inline">\(\underline{r}_t\)</span> already contains all the information to compute <span class="math inline">\(n_t\)</span> since <span id="eq-freq_n_sum_r"><span class="math display">\[
n_t = \sum_{b=1}^B r_t^{(b)}.
\tag{49}\]</span></span> But adding the variable allows us to factorize <a href="#eq-freq_lambda_bayes">Equation&nbsp;48</a> as <span id="eq-freq_lambda_bayes_factorized"><span class="math display">\[
\pi(\underline{\lambda}_t \mid n_t, \underline{r}_t) \propto
\pi(\underline{r}_t \mid n_t, \underline{\lambda}_t)
\pi(n_t \mid \underline{\lambda}_t)
\pi(\underline{\lambda}_t)
\tag{50}\]</span></span> We then have <span id="eq-freq_r_bayes"><span class="math display">\[
\underline{r}_t \mid n_t, \underline{\lambda}_t \sim
\operatorname{Multinomial}(n_t, \underline{f}_t(\underline{\lambda}_t)).
\tag{51}\]</span></span> Furthermore, we have <span class="math display">\[
n_t \mid \underline{\lambda}_t \sim
\operatorname{Poiss}\left(\sum_{b=1}^B \lambda_t^{(b)}\right).
\]</span>{#eq=freq_n_bayes} Finally, for our prior <span class="math inline">\(\pi(\underline{\lambda}_t)\)</span>, we first assume each parameter is independent, i.e., <span class="math display">\[
\pi(\underline{\lambda}_t) = \prod_{b=1}^B \pi(\lambda_t^{(b)}).
\]</span> A reasonable prior for each <span class="math inline">\(\lambda_t^{(b)}\)</span> representing the expected number of reads for barcode <span class="math inline">\(b\)</span> should span several orders of magnitude. Furthermore, we assume that no barcode in the dataset ever goes extinct. Thus, no frequency can equal zero, facilitating the computation of the log frequency ratios needed to infer the relative fitness. The log-normal distribution satisfies these constraints; therefore, for the prior, we assume <span id="eq-freq_lambda_prior"><span class="math display">\[
\lambda_t^{(b)} \sim
\log\mathcal{N}(\mu_{\lambda_t^{(b)}}, \sigma_{\lambda_t^{(b)}}),
\tag{52}\]</span></span> with <span class="math inline">\(\mu_{\lambda_t^{(b)}}, \sigma_{\lambda_t^{(b)}}\)</span> as the user-defined parameters that characterize the prior distribution.</p>
<section id="summary" class="level4">
<h4 class="anchored" data-anchor-id="summary">Summary</h4>
<p>Putting all the pieces developed in this section together gives a term for our inference of the form <span id="eq-freq_final_01"><span class="math display">\[
\pi(\underline{\underline{F}} \mid \underline{\underline{R}}) \propto
\prod_{t=1}^T\left\{
    \pi(\underline{r}_t \mid n_t, \underline{\lambda}_t)
    \pi(n_t \mid \underline{\lambda}_t)
    \left[
        \prod_{b=1}^B \pi(\lambda_t^{(b)})
    \right]
\right\}
\tag{53}\]</span></span> where <span id="eq-freq_final_02"><span class="math display">\[
\underline{r}_t \mid n_t, \underline{\lambda}_t \sim
\operatorname{Multinomial}(n_t, \underline{f}_t(\underline{\lambda}_t)),
\tag{54}\]</span></span> <span id="eq-freq_final_03"><span class="math display">\[
n_t \mid \underline{\lambda}_t \sim
\operatorname{Poiss}\left(\sum_{b=1}^B \lambda_t^{(b)}\right).
\tag{55}\]</span></span> and <span id="eq-freq_final_04"><span class="math display">\[
\lambda_t^{(b)} \sim
\log\mathcal{N}(\mu_{\lambda_t^{(b)}}, \sigma_{\lambda_t^{(b)}}),
\tag{56}\]</span></span></p>
</section>
</section>
<section id="sec-bayes_meanfit" class="level3">
<h3 class="anchored" data-anchor-id="sec-bayes_meanfit">Population mean fitness uncertainty <span class="math inline">\(\pi(\underline{\bar{s}}_T \mid \underline{\underline{F}}, \underline{\underline{R}})\)</span></h3>
<p>Next, we turn our attention to the problem of determining the population mean fitnesses <span class="math inline">\(\underline{\bar{s}}_T\)</span>. First, we notice that our fitness model in <a href="#eq-fitness">Equation&nbsp;3</a> does not include the value of the raw reads. They enter the calculation indirectly through the inference of the frequency values we developed in <a href="#sec-bayes_freq">Section&nbsp;5.2.1</a>. This means that we can remove the conditioning of the value of <span class="math inline">\(\underline{\bar{s}}_T\)</span> on the number of reads, obtaining a simpler probability function <span id="eq-meanfit_noreads"><span class="math display">\[
\pi(
    \underline{\bar{s}}_T \mid
    \underline{\underline{F}}, \underline{\underline{R}}
) =
\pi(
    \underline{\bar{s}}_T \mid
    \underline{\underline{F}}
).
\tag{57}\]</span></span> Moreover, our fitness model does not directly explain how the population mean fitness evolves over time. In other words, our model cannot explicitly compute the population mean fitness at time <span class="math inline">\(t+1\)</span> from the information we have about time <span class="math inline">\(t\)</span>. Given this model limitation, we are led to assume that we must infer each <span class="math inline">\(\bar{s}_t\)</span> independently. Expressing this for our inference results in <span id="eq-meanfit_indep"><span class="math display">\[
\pi(
    \underline{\bar{s}}_T \mid
    \underline{\underline{F}}
) =
\prod_{t=1}^{T-1} \pi(\bar{s}_t \mid \underline{f}_t, \underline{f}_{t+1}),
\tag{58}\]</span></span> where we split our matrix <span class="math inline">\(\underline{\underline{F}}\)</span> for each time point and only kept the conditioning on the relevant frequencies needed to compute the mean fitness at time <span class="math inline">\(t\)</span>.</p>
<p>Although our fitness model in <a href="#eq-fitness">Equation&nbsp;3</a> also includes the relative fitness <span class="math inline">\(s^{(m)}\)</span>, to infer the population mean fitness we only utilize data from the neutral lineages that, by definition, have a relative fitness <span class="math inline">\(s^{(n)} = 0\)</span>. Therefore, the conditioning on <a href="#eq-meanfit_indep">Equation&nbsp;58</a> can be further simplified by only keeping the frequencies of the neutral lineages, i.e., <span id="eq-meanfit_indep_neutral"><span class="math display">\[
\pi(\bar{s}_t \mid \underline{f}_t, \underline{f}_{t+1}) =
\pi(\bar{s}_t \mid \underline{f}_t^N, \underline{f}_{t+1}^N).
\tag{59}\]</span></span></p>
<p>Recall that in <a href="#sec-fitness_model">Section&nbsp;2.3</a> we emphasized that the frequencies <span class="math inline">\(f_t^{(n)}\)</span> do not represent the true frequency of a particular lineage in the population but rather a “normalized number of cells.” Therefore, it is safe to assume each of the <span class="math inline">\(N\)</span> neutral lineages’ frequencies is changing independently. The correlation of how increasing the frequency of one lineage will decrease the frequency of others is already captured in the model presented in <a href="#sec-bayes_freq">Section&nbsp;5.2.1</a>. Thus, we write <span id="eq-meanfit_indep_lineages"><span class="math display">\[
\pi(\bar{s}_t \mid \underline{f}_t^N, \underline{f}_{t+1}^N) =
\prod_{n=1}^N \pi(\bar{s}_t \mid f_t^{(n)}, f_{t+1}^{(n)}).
\tag{60}\]</span></span></p>
<p>Now, we can focus on one of the terms on the right-hand side of <a href="#eq-meanfit_indep_lineages">Equation&nbsp;60</a>. Writing Bayes theorem results in <span id="eq-meanfit_bayes"><span class="math display">\[
\pi(\bar{s}_t \mid f_t^{(n)}, f_{t+1}^{(n)}) \propto
\pi(f_t^{(n)}, f_{t+1}^{(n)} \mid \bar{s}_t) \pi(\bar{s}_t).
\tag{61}\]</span></span> Notice the likelihood defines the joint distribution of neutral barcode frequencies conditioned on the population mean fitness. However, rewriting our fitness model in <a href="#eq-fitness">Equation&nbsp;3</a> for a neutral lineage to leave frequencies on one side and fitness on the other results in <span id="eq-fitness_ratio_neutral"><span class="math display">\[
\frac{f_{t+1}^{(n)}}{f_t^{(n)}} = \mathrm{e}^{- \bar{s}_t\tau}.
\tag{62}\]</span></span> <a href="#eq-fitness_ratio_neutral">Equation&nbsp;62</a> implies that our fitness model only relates <strong>the ratio</strong> of frequencies and not the individual values. To get around this complication, we define <span id="eq-gamma_def"><span class="math display">\[
\gamma_t^{(b)} \equiv \frac{f_{t+1}^{(b)}}{f_t^{(b)}},
\tag{63}\]</span></span> as the ratio of frequencies between two adjacent time points for any barcode <span class="math inline">\(b\)</span>. This allows us to rewrite the joint distribution <span class="math inline">\(\pi(f_t^{(n)}, f_{t+1}^{(n)} \mid \bar{s}_t)\)</span> as <span id="eq-joint_freq_gamma"><span class="math display">\[
\pi(f_t^{(n)}, f_{t+1}^{(n)} \mid \bar{s}_t) =
\pi(f_t^{(n)}, \gamma_{t}^{(n)} \mid \bar{s}_t).
\tag{64}\]</span></span> Let us rephrase this subtle but necessary change of variables since it is a key part of the inference problem: our series of independence assumptions lead us to <a href="#eq-meanfit_bayes">Equation&nbsp;61</a> that relates the value of the population mean fitness <span class="math inline">\(\bar{s}_t\)</span> to the frequency of a neutral barcode at times <span class="math inline">\(t\)</span> and <span class="math inline">\(t+1\)</span>. However, as shown in <a href="#eq-fitness_ratio_neutral">Equation&nbsp;62</a>, our model functionally relates the ratio of frequencies—that we defined as <span class="math inline">\(\gamma_t^{(n)}\)</span>—and not the independent frequencies to the mean fitness. Therefore, instead of writing for the likelihood the joint distribution of the frequency values at times <span class="math inline">\(t\)</span> and <span class="math inline">\(t+1\)</span> conditioned on the mean fitness, we write the joint distribution of the barcode frequency at time <span class="math inline">\(t\)</span> and the ratio of the frequencies. These <strong>must be</strong> equivalent joint distributions since there is a one-to-one mapping between <span class="math inline">\(\gamma_t^{(n)}\)</span> and <span class="math inline">\(f_{t+1}^{(n)}\)</span> for a given value of <span class="math inline">\(f_t^{(n)}\)</span>. Another way to phrase this is to say that knowing the frequency at time <span class="math inline">\(t\)</span> and at time <span class="math inline">\(t+1\)</span> provides the same amount of information as knowing the frequency at time <span class="math inline">\(t\)</span> and the ratio of the frequencies. This is because if we want to obtain <span class="math inline">\(f_{t+1}^{(n)}\)</span> given this information, we simply compute <span id="eq-meanfit_ft+1_gamma"><span class="math display">\[
f_{t+1}^{(n)} = \gamma_t^{(n)} f_t^{(n)}.
\tag{65}\]</span></span></p>
<p>The real advantage of rewriting the joint distribution as in <a href="#eq-joint_freq_gamma">Equation&nbsp;64</a> comes from splitting this joint distribution as a product of conditional distributions of the form <span id="eq-joint_to_prod_gamma"><span class="math display">\[
\pi(f_t^{(n)}, \gamma_{t}^{(n)} \mid \bar{s}_t) =
\pi(f_t^{(n)} \mid \gamma_{t}^{(n)}, \bar{s}_t)
\pi(\gamma_{t}^{(n)} \mid \bar{s}_t).
\tag{66}\]</span></span> Written in this form, we can finally propose a probabilistic model for how the mean fitness relates to the frequency ratios we determine in our experiments. The second term on the right-hand side of <a href="#eq-joint_to_prod_gamma">Equation&nbsp;66</a> relates how the determined frequency ratio <span class="math inline">\(\gamma_t^{(b)}\)</span> relates to the mean fitness <span class="math inline">\(\bar{s}_t\)</span>. From <a href="#eq-fitness_ratio_neutral">Equation&nbsp;62</a> and <a href="#eq-gamma_def">Equation&nbsp;63</a>, we can write <span id="eq-log_ratio_neutral"><span class="math display">\[
\ln \gamma_t^{(n)} = - \bar{s}_t + \varepsilon_t^{(n)},
\tag{67}\]</span></span> where, for simplicity, we set <span class="math inline">\(\tau = 1\)</span>. Note that we added an extra term, <span class="math inline">\(\varepsilon_t^{(n)}\)</span>, characterizing the deviations of the measurements from the theoretical model. We assume these errors are normally distributed with mean zero and some standard deviation <span class="math inline">\(\sigma_t\)</span>, implying that <span id="eq-log_gamma_normal"><span class="math display">\[
\ln \gamma_t^{(n)} \mid \bar{s}_t, \sigma_t  \sim
\mathcal{N}\left(-\bar{s}_t, \sigma_t \right),
\tag{68}\]</span></span> where we include the nuisance parameter <span class="math inline">\(\sigma_t\)</span> to be determined. If we assume the log frequency ratio is normally distributed, this implies the frequency ratio itself is distributed log-normal. This means that <span id="eq-meanfit_likelihood"><span class="math display">\[
\gamma_t^{(n)} \mid \bar{s}_t, \sigma_t  \sim
\log \mathcal{N}\left(-\bar{s}_t, \sigma_t \right).
\tag{69}\]</span></span> Having added the nuisance parameter <span class="math inline">\(\sigma_t\)</span> implies that we must update <a href="#eq-meanfit_bayes">Equation&nbsp;61</a> to <span id="eq-meanfit_bayes_full"><span class="math display">\[
\pi(\bar{s}_t, \sigma_t \mid f_t^{(n)}, f_{t+1}^{(n)}) \propto
\pi(f_t^{(n)}, \gamma_t^{(n)} \mid \bar{s}_t, \sigma_t)
\pi(\bar{s}_t) \pi(\sigma_t),
\tag{70}\]</span></span> where we assume the prior for each parameter is independent, i.e., <span id="eq-meanfit_indep_prior"><span class="math display">\[
\pi(\bar{s}_t, \sigma_t) = \pi(\bar{s}_t) \pi(\sigma_t).
\tag{71}\]</span></span> For numerical stability, we will select weakly-informative priors for both of these parameters. In the case of the nuisance parameter <span class="math inline">\(\sigma_t\)</span>, the prior must be restricted to positive values only, since standard deviations cannot be negative.</p>
<p>For the first term on the right-hand side of <a href="#eq-joint_to_prod_gamma">Equation&nbsp;66</a>, <span class="math inline">\(\pi(f_t^{(n)} \mid \gamma_{t}^{(n)}, \bar{s}_t)\)</span>, we remove the conditioning on the population mean fitness since it does not add any information on top of what the frequency ratio <span class="math inline">\(\gamma_t^{(n)}\)</span> already gives. Therefore, we have <span id="eq-freq_cond_gamma"><span class="math display">\[
\pi(f_t^{(n)} \mid \gamma_{t}^{(n)}, \bar{s}_t) =
\pi(f_t^{(n)} \mid \gamma_{t}^{(n)}).
\tag{72}\]</span></span> The right-hand side of <a href="#eq-freq_cond_gamma">Equation&nbsp;72</a> asks us to compute the probability of observing a frequency value <span class="math inline">\(f_t^{(n)}\)</span> given that we get to observe the ratio <span class="math inline">\(\gamma_{t}^{(n)}\)</span>. If the ratio happened to be <span class="math inline">\(\gamma_{t}^{(n)} = 2\)</span>, we could have <span class="math inline">\(f_{t+1}^{(n)} = 1\)</span> and <span class="math inline">\(f_{t+1}^{(n)} = 0.5\)</span>, for example. Although, it would be equally likely that <span class="math inline">\(f_{t+1}^{(n)} = 0.6\)</span> and <span class="math inline">\(f_{t+1}^{(n)} = 0.3\)</span> or <span class="math inline">\(f_{t+1}^{(n)} = 0.1\)</span> and <span class="math inline">\(f_{t+1}^{(n)} = 0.05\)</span> for that matter. If we only get to observe the frequency ratio <span class="math inline">\(\gamma_t^{(n)}\)</span>, we know that the numerator <span class="math inline">\(f_{t+1}^{(n)}\)</span> can only take values between zero and one, all of them being equally likely given only the information on the ratio. As a consequence, the value of the frequency in the denominator <span class="math inline">\(f_{t}^{(n)}\)</span> is restricted to fall in the range <span id="eq-freq_given_gamma_range"><span class="math display">\[
f_{t}^{(n)} \in \left(0, \frac{1}{\gamma_t^{(n)}} \right].
\tag{73}\]</span></span> A priori, we do not have any reason to favor any value over any other, therefore it is natural to write <span id="eq-f_given_gamma_uniform"><span class="math display">\[
f_t^{(n)} \mid \gamma_t^{(n)} \sim
\operatorname{Uniform}\left( 0, \frac{1}{\gamma_t^{(n)}} \right).
\tag{74}\]</span></span></p>
<section id="summary-1" class="level4">
<h4 class="anchored" data-anchor-id="summary-1">Summary</h4>
<p>Putting all the pieces we have developed in this section together results in an inference for the population mean fitness values of the form <span id="eq-meanfit_full_inference"><span class="math display">\[
\pi(
    \underline{\bar{s}}_T, \underline{\sigma}_T \mid \underline{\underline{F}}
) \propto
\prod_{t=1}^{T-1} \left\{
    \prod_{n=1}^N \left[
        \pi(f_t^{(n)} \mid \gamma_t^{(n)})
        \pi(\gamma_t^{(n)} \mid \bar{s}_t, \sigma_t)
    \right]
    \pi(\bar{s}_t) \pi(\sigma_t)
\right\},
\tag{75}\]</span></span> where we have <span id="eq-meanfit_eq1"><span class="math display">\[
f_t^{(n)} \mid \gamma_t^{(n)} \sim
\operatorname{Uniform} \left(0, \frac{1}{\gamma_t^{(n)}} \right),
\tag{76}\]</span></span> <span id="eq-meanfit_eq2"><span class="math display">\[
\gamma_t^{(n)} \mid \bar{s}_t, \sigma_t \sim
\log\mathcal{N}(\bar{s}_t, \sigma_t),
\tag{77}\]</span></span> <span id="eq-meanfit_eq3"><span class="math display">\[
\bar{s}_t \sim \mathcal{N}(0, \sigma_{\bar{s}_t}),
\tag{78}\]</span></span> and <span id="eq-meanfit_eq4"><span class="math display">\[
\sigma_t \sim \log\mathcal{N}(\mu_{\sigma_t}, \sigma_{\sigma_t}),
\tag{79}\]</span></span> where <span class="math inline">\(\sigma_{\bar{s}_t}\)</span>, <span class="math inline">\(\mu_{\sigma_t}\)</span>, and <span class="math inline">\(\sigma_{\sigma_t}\)</span> are user-defined parameters.</p>
</section>
</section>
<section id="sec-bayes_mutfit" class="level3">
<h3 class="anchored" data-anchor-id="sec-bayes_mutfit">Mutant relative fitness uncertainty <span class="math inline">\(\pi(\underline{s}^M \mid \underline{\bar{s}}_T, \underline{\underline{F}}, \underline{\underline{R}})\)</span></h3>
<p>The last piece of our inference is the piece that we care about the most: the probability distribution of all the mutants’ relative fitness, given the inferred population mean fitness and the frequencies. First, we assume that all fitness values are independent of each other. This allows us to write <span id="eq-mutfit_indep"><span class="math display">\[
\pi(
    \underline{s}^M \mid
    \underline{\bar{s}}_T, \underline{\underline{F}}, \underline{\underline{R}}
) =
\prod_{m=1}^M \pi(
    s^{(m)} \mid
    \underline{\bar{s}}_T, \underline{\underline{F}}, \underline{\underline{R}}
).
\tag{80}\]</span></span> Furthermore, as was the case with the population mean fitness, our fitness model relates frequencies, not raw reads. Moreover, the fitness value of mutant <span class="math inline">\(m\)</span> only depends on the frequencies of such mutant. Therefore, we can simplify the conditioning to <span id="eq-mutfit_cond_simple"><span class="math display">\[
\pi(
    s^{(m)} \mid
    \underline{\bar{s}}_T, \underline{\underline{F}}, \underline{\underline{R}}
) =
\pi(s^{(m)} \mid \underline{\bar{s}}_T, \underline{f}^{(m)}),
\tag{81}\]</span></span> where <span id="eq-mutfit_timeseries_def"><span class="math display">\[
\underline{f}^{(m)} = (f_0^{(m)}, f_1^{(m)}, \ldots, f_T^{(m)})^\dagger,
\tag{82}\]</span></span> is the vector containing the frequency time series for mutant <span class="math inline">\(m\)</span>. Writing Bayes’ theorem for the right-hand side of <a href="#eq-mutfit_cond_simple">Equation&nbsp;81</a> results in <span id="eq-mutfit_bayes"><span class="math display">\[
\pi(s^{(m)} \mid \underline{\bar{s}}_T, \underline{f}^{(m)}) \propto
\pi(\underline{f}^{(m)} \mid \underline{\bar{s}}_T, s^{(m)})
\pi(s^{(m)} \mid \underline{\bar{s}}_T).
\tag{83}\]</span></span> Notice the conditioning on the mean fitness values <span class="math inline">\(\underline{\bar{s}}_T\)</span> is not inverted since we already inferred these values.</p>
<p>Following the logic used in <a href="#sec-bayes_meanfit">Section&nbsp;5.2.2</a>, let us define <span id="eq-gamma_vec_def"><span class="math display">\[
\underline{\gamma}^{(m)} =
(\gamma_0^{(m)}, \gamma_1^{(m)}, \ldots, \gamma_{T-1}^{m})^\dagger,
\tag{84}\]</span></span> where each entry <span class="math inline">\(\gamma_t^{(m)}\)</span> is defined by <a href="#eq-gamma_def">Equation&nbsp;63</a>. In the same way we rewrote the joint distribution between two adjacent time point frequencies to the joint distribution between one of the frequencies and the ratio of both frequencies in <a href="#eq-joint_freq_gamma">Equation&nbsp;64</a>, we can rewrite the joint distribution of the frequency time series for mutant <span class="math inline">\(m\)</span> as <span id="eq-joint_freq_gammas_mutfit"><span class="math display">\[
\pi(\underline{f}^{(m)} \mid \underline{\bar{s}}_T, s^{(m)}) =
\pi(f_0^{(m)}, \underline{\gamma}^{(m)} \mid \underline{\bar{s}}_T, s^{(m)}).
\tag{85}\]</span></span> One can think about <a href="#eq-joint_freq_gammas_mutfit">Equation&nbsp;85</a> as saying that knowing the individual frequencies at each time point contain equivalent information as knowing the initial frequency and the subsequent ratios of frequencies. This is because if we want to know the value of <span class="math inline">\(f_1^{(m)}\)</span> given the ratios, we only need to compute <span id="eq-f1_from_ratio"><span class="math display">\[
f_1^{(m)} = \gamma_0^{(m)} f_0^{(m)}.
\tag{86}\]</span></span> Moreover, if we want to know <span class="math inline">\(f_2^{(m)}\)</span>, we have <span id="eq-f2_from_ratio"><span class="math display">\[
f_2^{(m)} = \gamma_1^{(m)} f_1^{(m)} =
\gamma_1^{(m)} \left(\gamma_0^{(m)} f_0^{(m)}\right),
\tag{87}\]</span></span> and so on. We can then write the joint distribution on the right-hand side of <a href="#eq-joint_freq_gammas_mutfit">Equation&nbsp;85</a> as a product of conditional distributions of the form <span id="eq-mutfit_joint_to_cond"><span class="math display">\[
\begin{aligned}
\pi(f_0^{(m)}, \underline{\gamma}^{(m)} \mid \underline{\bar{s}}_T, s^{(m)}) =
&amp;\pi(
    f_0^{(m)} \mid
    \gamma_0^{(m)}, \ldots, \gamma_{T-1}^{(m)}, \underline{\bar{s}}_T, s^{(m)}
) \times \\
&amp;\pi(
    \gamma_0^{(m)} \mid
    \gamma_1^{(m)}, \ldots, \gamma_{T-1}^{(m)}, \underline{\bar{s}}_T, s^{(m)}
) \times \\
&amp;\pi(
    \gamma_1^{(m)} \mid
    \gamma_2^{(m)}, \ldots, \gamma_{T-1}^{(m)}, \underline{\bar{s}}_T, s^{(m)}
) \times \\
&amp;\vdots \\
&amp;\pi(
    \gamma_{T-2}^{(m)} \mid \gamma_{T-1}^{(m)}, \underline{\bar{s}}_T, s^{(m)}
) \times \\
&amp;\pi(\gamma_{T-1}^{(m)} \mid \underline{\bar{s}}_T, s^{(m)}).
\end{aligned}
\tag{88}\]</span></span> Writing the fitness model in <a href="#eq-fitness">Equation&nbsp;3</a> as <span class="math display">\[
\gamma_t^{(m)} = \frac{f_{t+1}^{(m)}}{f_t^{(m)}} =
\mathrm{e}^{(s^{(m)} - s_t)\tau},
\]</span> reveals that the value of each of the ratios <span class="math inline">\(\gamma_t^{(m)}\)</span> only depends on the corresponding fitness value <span class="math inline">\(\bar{s}_t\)</span> and the relative fitness <span class="math inline">\(s^{(m)}\)</span>. Therefore, we can remove most of the conditioning on the right-hand side of <a href="#eq-mutfit_joint_to_cond">Equation&nbsp;88</a>, resulting in a much simpler joint distribution of the form <span id="eq-mutfit_joint_to_cond_simple"><span class="math display">\[
\begin{aligned}
\pi(f_0^{(m)}, \underline{\gamma}^{(m)} \mid \underline{\bar{s}}_T, s^{(m)}) =
&amp;\pi(f_0^{(m)} \mid \gamma_0^{(m)}) \times \\
&amp;\pi(\gamma_0^{(m)} \mid \bar{s}_0, s^{(m)}) \times \\
&amp;\pi(\gamma_1^{(m)} \mid \bar{s}_1, s^{(m)}) \times \\
&amp;\vdots \\
&amp;\pi(\gamma_{T-2}^{(m)} \mid \bar{s}_{T-2}, s^{(m)}) \times \\
&amp;\pi(\gamma_{T-1}^{(m)} \mid \bar{s}_{T-1}, s^{(m)}),
\end{aligned}
\tag{89}\]</span></span> where for the first term on the right-hand side of <a href="#eq-mutfit_joint_to_cond_simple">Equation&nbsp;89</a> we apply the same logic as in <a href="#eq-freq_cond_gamma">Equation&nbsp;72</a> to remove all other dependencies. We emphasize that although <a href="#eq-mutfit_joint_to_cond_simple">Equation&nbsp;89</a> looks like a series of independent inferences, the value of the relative fitness <span class="math inline">\(s^{(m)}\)</span> is shared among all of them. This means that the parameter is not inferred individually for each time point, resulting in different estimates of the parameter, but each time point contributes independently to the inference of a single estimate of <span class="math inline">\(s^{(m)}\)</span>.</p>
<p>Using equivalent arguments to those in <a href="#sec-bayes_meanfit">Section&nbsp;5.2.2</a>, we assume <span class="math display">\[
f_0^{(m)} \mid \gamma_0^{(m)} \sim
\operatorname{Uniform}\left(0, \frac{1}{\gamma_0^{(m)}} \right),
\]</span> and <span id="eq-mutfit_lognormal"><span class="math display">\[
\gamma_t^{(m)} \mid \bar{s}_t, s^{(m)}, \sigma^{(m)} \sim
\log\mathcal{N}\left(s^{(m)} - \bar{s}_t, \sigma^{(m)} \right),
\tag{90}\]</span></span> where we add the nuisance parameter <span class="math inline">\(\sigma^{(m)}\)</span> to the inference. Notice that this parameter is not indexed by time. This means that we assume the deviations from the theoretical prediction do not depend on time, but only on the mutant. Adding the nuisance parameter demands us to update <a href="#eq-mutfit_bayes">Equation&nbsp;83</a> to <span id="eq-mutfit_bayes_full"><span class="math display">\[
\pi(
    s^{(m)}, \sigma^{(m)} \mid \underline{\bar{s}}_T, \underline{f}^{(m)}
) \propto
\pi(\underline{f}^{(m)} \mid \underline{\bar{s}}_T, s^{(m)}, \sigma^{(m)})
\pi(s^{(m)}) \pi(\sigma^{(m)}),
\tag{91}\]</span></span> where we assume independent priors for both parameters. We also removed the conditioning on the values of the mean fitness as knowing such values does not change our prior information about the possible range of values that the parameters can take. As with the priors on <a href="#sec-bayes_meanfit">Section&nbsp;5.2.2</a>, we will assign weakly-informative priors to these parameters.</p>
<section id="summary-2" class="level4">
<h4 class="anchored" data-anchor-id="summary-2">Summary</h4>
<p>With all pieces in place, we write the full inference of the relative fitness values as <span id="eq-mutfit_full_inference"><span class="math display">\[
\pi(
    \underline{s}^M ,\underline{\sigma}^M \mid
    \underline{\bar{s}}_T, \underline{\underline{F}}
) \propto
\prod_{m=1}^M \left\{
    \pi(f_0^{(m)} \mid \gamma_0^{(m)})
    \prod_{t=0}^{T-1} \left[
        \pi(\gamma_t^{(m)} \mid \bar{s}_t, s^{(m)}, \sigma^{(m)})
    \right]
    \pi(s^{(m)}) \pi(\sigma^{(m)})
\right\},
\tag{92}\]</span></span> where <span id="eq-mutfit_eq1"><span class="math display">\[
f_0^{(m)} \mid \gamma_0^{(m)} \sim
\operatorname{Uniform}\left(0, \frac{1}{\gamma_0^{(m)}} \right),
\tag{93}\]</span></span> <span id="eq-mutfit_eq2"><span class="math display">\[
\gamma_t^{(m)} \mid \bar{s}_t, s^{(m)}, \sigma^{(m)} \sim
\log\mathcal{N}\left(s^{(m)} - \bar{s}_t, \sigma^{(m)} \right),
\tag{94}\]</span></span> <span id="eq-mutfit_eq3"><span class="math display">\[
s^{(m)} \sim \mathcal{N}(0, \sigma_{s^{(m)}}),
\tag{95}\]</span></span> and <span id="eq-mutfit_eq4"><span class="math display">\[
\sigma^{(m)} \sim \log\mathcal{N}(\mu_{\sigma^{(m)}}, \sigma_{\sigma^{(m)}}),
\tag{96}\]</span></span> where <span class="math inline">\(\sigma_{s^{(m)}}\)</span>, <span class="math inline">\(\mu_{\sigma^{(m)}}\)</span>, and <span class="math inline">\(\sigma_{\sigma^{(m)}}\)</span> are user-defined parameters.</p>
<!-- Hierarchical models -->
</section>
</section>
<section id="sec-hierarchical_model" class="level3">
<h3 class="anchored" data-anchor-id="sec-hierarchical_model">Hierarchical models for multiple experimental replicates</h3>
<p>As detailed in <a href="#sec-replicates">Section&nbsp;2.7</a> of the main text, we define a Bayesian hierarchical model to analyze data from multiple experimental replicates. The implementation requires only slightly modifying the base model detailed in the previous sections. The hierarchical model defines a hyper-fitness parameter <span class="math inline">\(\theta^{(m)}\)</span> for every non-neutral barcode. We can thus collect all of the <span class="math inline">\(M\)</span> hyperparameters in an array of the form <span id="eq-hier_hyper_vector"><span class="math display">\[
\underline{\theta}^M = (\theta^{(1)}, \ldots, \theta^{(M)})^\dagger.
\tag{97}\]</span></span> Our data now consists of a series of matrices <span class="math inline">\(\underline{\underline{R}}_{[j]}\)</span>, where the subindex <span class="math inline">\([j]\)</span> refers to the <span class="math inline">\(j\)</span>-th experimental replicate. These matrices need not have the same number of rows, as the time points measured for each replicate can vary. The statistical model we must define is then of the form <span id="eq-hier_bayes_full"><span class="math display">\[
\begin{aligned}
\pi(
    \underline{\theta}^M,
    \{\underline{s}^M_{[j]}\},
    \{\underline{\bar{s}}_{T[j]}\},
    \{\underline{\underline{F}}_{[j]}\} \mid
    \{\underline{\underline{R}}_{[j]}\}
) \propto \;
&amp;\pi(
    \{\underline{\underline{R}}_{[j]}\} \mid
    \underline{\theta}^M,
    \{\underline{s}^M_{[j]}\},
    \{\underline{\bar{s}}_{T[j]}\},
    \{\underline{\underline{F}}_{[j]}\}
) \times \\\
&amp;\pi(
    \underline{\theta}^M,
    \{\underline{s}^M_{[j]}\},
    \{\underline{\bar{s}}_{T[j]}\},
    \{\underline{\underline{F}}_{[j]}\}
)
\end{aligned}
\tag{98}\]</span></span> where the parameters within curly braces with subindex <span class="math inline">\([j]\)</span> indicate one set of parameters per experimental replicate. For example, <span id="eq-hier_array"><span class="math display">\[
\{\underline{s}^M_{[j]}\} = \{
    \underline{s}^M_{[1]}, \underline{s}^M_{[2]}, \ldots \underline{s}^M_{[E]}
\},
\tag{99}\]</span></span> where <span class="math inline">\(E\)</span> is the number of experimental replicates.</p>
<p>Given the dependencies between the variables, we can factorize <a href="#eq-hier_bayes_full">Equation&nbsp;98</a> to be of the form <span id="eq-hier_factorized"><span class="math display">\[
\begin{aligned}
\pi(
    \underline{\theta}^M,
    \{\underline{s}^M_{[j]}\},
    \{\underline{\bar{s}}_{T[j]}\},
    \{\underline{\underline{F}}_{[j]}\} \mid
    \{\underline{\underline{R}}_{[j]}\}
) =
&amp;\pi(
    \underline{\theta}^M,
    \{\underline{s}^M_{[j]}\} \mid
    \{\underline{\bar{s}}_{T[j]}\},
    \{\underline{\underline{F}}_{[j]}\}
) \times \\
&amp;\pi(
    \{\underline{\bar{s}}_{T[j]}\} \mid
    \{\underline{\underline{F}}_{[j]}\}
) \times \\
&amp;\pi(
    \{\underline{\underline{F}}_{[j]}\} \mid
    \{\underline{\underline{R}}_{[j]}\}
)
\end{aligned}
\tag{100}\]</span></span> Furthermore, the hierarchical structure only connects the replicates via the relative fitness parameters. This means that the population mean fitness values and the frequencies can be independently inferred for each dataset. This allows us to rewrite the right-hand side of <a href="#eq-hier_factorized">Equation&nbsp;100</a> as <span id="eq-hier_bayes_factorized_replicate"><span class="math display">\[
\begin{aligned}
\pi(
    \underline{\theta}^M,
    \{\underline{s}^M_{[j]}\},
    \{\underline{\bar{s}}_{T[j]}\},
    \{\underline{\underline{F}}_{[j]}\} \mid
    \{\underline{\underline{R}}_{[j]}\}
) =
&amp;\pi(
    \underline{\theta}^M,
    \{\underline{s}^M_{[j]}\} \mid
    \{\underline{\bar{s}}_{T[j]}\},
    \{\underline{\underline{F}}_{[j]}\}
) \times \\
&amp;\prod_{j=1}^E \left[
    \pi(
        \underline{\bar{s}}_{T[j]} \mid
        \underline{\underline{F}}_{[j]}
    )
    \pi(
        \underline{\underline{F}}_{[j]} \mid
        \underline{\underline{R}}_{[j]}
    )
\right].
\end{aligned}
\tag{101}\]</span></span> The terms inside the square brackets in <a href="#eq-hier_bayes_factorized_replicate">Equation&nbsp;101</a> take the same functional form as those derived in <a href="#sec-bayes_meanfit">Section&nbsp;5.2.2</a> and <a href="#sec-bayes_freq">Section&nbsp;5.2.1</a>. Therefore, to implement the desired hierarchical model, we only need to focus on the first term on the right-hand side of <a href="#eq-hier_bayes_factorized_replicate">Equation&nbsp;101</a>. A way to think about the structure of the hierarchical model is as follows: imagine each genotype as a “<em>true</em>” relative fitness value. However, every time we perform an experiment, small variations in the biotic and abiotic conditions—also known as batch effects—might result in small deviations from this value. We model this by defining a distribution for the hyper-fitness parameter—the ground truth we are interested in—and having each experimental replicate sample from this hyper-parameter distribution to determine the “<em>local</em>” fitness value. The wider the hyper-parameter distribution is the more variability between experimental replicates.</p>
<p>Writing Bayes’ theorem for the first term in <a href="#eq-hier_bayes_factorized_replicate">Equation&nbsp;101</a> results in <span id="eq-hier_bayes"><span class="math display">\[
\pi(
    \underline{\theta}^M,
    \{\underline{s}^M_{[j]}\} \mid
    \{\underline{\underline{F}}_{[j]}\},
    \{\underline{\bar{s}}_{T[j]}\}
) \propto
\pi(
    \{\underline{\underline{F}}_{[j]}\} \mid
    \underline{\theta}^M,
    \{\underline{s}^M_{[j]}\},
    \{\underline{\bar{s}}_{T[j]}\}
)
\pi(
    \underline{\theta}^M,
    \{\underline{s}^M_{[j]}\} \mid
    \{\underline{\bar{s}}_{T[j]}\}
),
\tag{102}\]</span></span> where we leave the conditioning on the population mean fitness as we did in <a href="#sec-bayes_mutfit">Section&nbsp;5.2.3</a>. This expression can be simplified in two ways. First, the frequency values for each experimental replicate depend directly on the local fitness values and the corresponding population mean fitness, as the relationship between experimental replicates only occurs through the relative fitness values. Therefore, we can write <span id="eq-hier_bayes_exp_indep"><span class="math display">\[
\pi(
    \underline{\theta}^M,
    \{\underline{s}^M_{[j]}\} \mid
    \{\underline{\underline{F}}_{[j]}\},
    \{\underline{\bar{s}}_{T[j]}\}
) \propto
\prod_{j=1}^E\left[
    \pi(
        \underline{\underline{F}}_{[j]} \mid
        \underline{s}^M_{[j]},
        \underline{\bar{s}}_{T[j]}
    )
\right]
\pi(
    \underline{\theta}^M,
    \{\underline{s}^M_{[j]}\} \mid
    \{\underline{\bar{s}}_{T[j]}\}
).
\tag{103}\]</span></span> Second, the relationship between the hyper-fitness and the local fitness values allows us to write their joint distribution as a conditional distribution where local fitness values depend on the global hyper-fitness value, obtaining <span id="eq-hier_bayes_exp_indep"><span class="math display">\[
\pi(
    \underline{\theta}^M,
    \{\underline{s}^M_{[j]}\} \mid
    \{\underline{\underline{F}}_{[j]}\},
    \{\underline{\bar{s}}_{T[j]}\}
) \propto
\prod_{j=1}^E\left[
    \pi(
        \underline{\underline{F}}_{[j]} \mid
        \underline{s}^M_{[j]},
        \underline{\bar{s}}_{T[j]}
    )
    \pi(
        \underline{s}^M_{[j]} \mid
        \underline{\theta}^M
    )
\right]
\pi(
    \underline{\theta}^M
).
\tag{104}\]</span></span> Notice we removed the conditioning on the population mean fitness as our prior expectations of what the global hyper-fitness or local fitness value might be do not depend on these nuisance parameters.</p>
<p>The first term on the right-hand side of <a href="#eq-hier_bayes_exp_indep">Equation&nbsp;104</a> takes the same functional form as the one derived in <a href="#sec-bayes_mutfit">Section&nbsp;5.2.3</a>. Therefore, all we are left with is to determine the functional forms for the hyper–prior <span class="math inline">\(\pi(\underline{\theta}^M)\)</span>, and the conditional probability <span class="math inline">\(\pi(\underline{s}^M_{[j]} \mid \underline{\theta}^M)\)</span>. In analogy to the assumptions used for the fitness values in <a href="#sec-bayes_mutfit">Section&nbsp;5.2.3</a>, we define the value of each hyper-fitness as independent. This means that we have <span id="eq-hier_hyper_prior"><span class="math display">\[
\pi(\underline{\theta}^M) = \prod_{m=1}^M \pi(\theta^{(m)}).
\tag{105}\]</span></span> Furthermore, we assume this prior is of the form <span id="eq-hier_hyper_prior_02"><span class="math display">\[
\theta^{(m)} \sim \mathcal{N}(\mu_{\theta^{(m)}}, \sigma_{\theta^{(m)}}),
\tag{106}\]</span></span> where <span class="math inline">\(\mu_{\theta^{(m)}}\)</span> and <span class="math inline">\(\sigma_{\theta^{(m)}}\)</span> are user-defined parameters encoding the prior expectations on the fitness values.</p>
<p>For the conditional distribution <span class="math inline">\(\pi(\underline{s}^M_{[j]} \mid \underline{\theta}^M)\)</span>, we use the so-called non-centered parametrization that avoids some of the intrinsic degeneracies associated with hierarchical models <span class="citation" data-cites="betancourt2013"><a href="#ref-betancourt2013" role="doc-biblioref">[13]</a></span>. We invite the reader to check <a href="https://betanalpha.github.io/assets/case_studies/hierarchical_modeling.html">this excellent blog</a> explaining the difficulties of working with hierarchical models. This non-centered parameterization implies that we introduce two nuisance parameters such that the local fitness <span class="math inline">\(s_{[j]}^{(m)}\)</span> is computed as <span id="eq-hier_noncentered"><span class="math display">\[
s_{[j]}^{(m)} = \theta^{(m)} +
(\tau_{[j]}^{(m)} \times \xi_{[j]}^{(m)}),
\tag{107}\]</span></span> where <span class="math inline">\(\theta^{(m)}\)</span> is the corresponding genotype hyper-fitness value, <span class="math inline">\(\xi_{[j]}^{(m)}\)</span> is a standard normal random variable, i.e., <span id="eq-hier_stdnormal"><span class="math display">\[
\xi_{[j]}^{(m)} \sim \mathcal{N}(0, 1),
\tag{108}\]</span></span> that allows deviations from the hyper-fitness value to be either positive or negative, and <span class="math inline">\(\tau_{[j]}^{(m)}\)</span> is a strictly positive random variable that characterizes the deviation of the local fitness value from the global hyper-fitness. We assume <span id="eq-hier_tau"><span class="math display">\[
\tau_{[j]}^{(m)} \sim \log\mathcal{N}(
    \mu_{\tau_{[j]}^{(m)}}, \sigma_{\tau_{[j]}^{(m)}}
)
\tag{109}\]</span></span> where <span class="math inline">\(\mu_{\tau_{[j]}^{(m)}}\)</span> and <span class="math inline">\(\sigma_{\tau_{[j]}^{(m)}}\)</span> are user-defined parameters capturing the expected magnitude of the batch effects.</p>
<!-- Prior selection -->
</section>
<section id="defining-prior-probabilities" class="level3">
<h3 class="anchored" data-anchor-id="defining-prior-probabilities">Defining prior probabilities</h3>
<p>One aspect commonly associated—in both positive and negative ways—to Bayesian analysis is the definition of prior probabilities. On the one hand, the naive textbook version of Bayesian analysis defines the prior as encoding the information we have about the inference in question before acquiring any data. This is the “ideal” use of priors that, whenever possible, should be implemented. On the other hand, for most practitioners of Bayesian statistics in the age of big data, the definition of prior becomes a tool to ensure the convergence of sampling algorithms such as MCMC <span class="citation" data-cites="gelman2017"><a href="#ref-gelman2017" role="doc-biblioref">[25]</a></span>. However, for our particular problem, although we deal with large amounts of data (inferences can be made for &gt; 10K barcodes over multiple time points, resulting in &gt; 100K parameters), each barcode has very little data, as they are measured only once per time point over &lt; 10 growth-dilution cycles. Furthermore, it is incredibly challenging to understand the noise sources related to culturing conditions, DNA extraction, library preparation, etc., and encode them into reasonable prior distributions.</p>
<p>Empirically, our approach for this work defined the priors based solely on the neutral lineage data, as they represent the only repeated measurements of a single genotype in our experimental design. We acknowledge that defining the priors after observing the data might be considered an incoherent inference. However, as expressed by <span class="citation" data-cites="gelman2017"><a href="#ref-gelman2017" role="doc-biblioref">[25]</a></span></p>
<blockquote class="blockquote">
<p>Incoherence is an unavoidable aspect of much real-world data analysis; and, indeed, one might argue that as scientists we learn the most from the anomalies and reassessments associated with episodes of incoherence.</p>
</blockquote>
<p>With this in mind, we leave it to the reader to judge the selection of priors. Furthermore, the software package associated with this work, <code>BarBay.jl</code>, is written so that users can experiment with different prior selection criteria that fit their needs. We strongly advocate that statistics should not be done in a black-box fit-all tool mindset but rather as a formal way to encode the assumptions behind the analysis, subject to constructive criticism. With this philosophical baggage behind us, let us now focus on how the priors used for this work were selected.</p>
<section id="naive-neutral-lineage-based-priors" class="level4">
<h4 class="anchored" data-anchor-id="naive-neutral-lineage-based-priors">Naive neutral lineage-based priors</h4>
<p>For the base model presented in this work, the user-defined prior parameters include the following:</p>
<ul>
<li><p>Prior on population mean fitness (one per pair of adjacent time points) <span id="eq-prior_meanfit_mean"><span class="math display">\[
\bar{s}_t \sim \mathcal{N}(\mu_{\bar{s}_t}, \sigma_{{\bar{s}_t}}).
\tag{110}\]</span></span></p></li>
<li><p>Prior on standard deviation associated with neutral lineages likelihood function (one per pair of adjacent time points) <span id="eq-prior_meanfit_var"><span class="math display">\[
\sigma_t \sim \log\mathcal{N}(\mu_{\sigma_t}, \sigma_{\sigma_t}).
\tag{111}\]</span></span></p></li>
<li><p>Prior on relative fitness (one per non-neutral barcode) <span id="eq-prior_fit_mean"><span class="math display">\[
s^{(m)} \sim \mathcal{N}(\mu_{s^{(m)}}, \sigma_{s^{(m)}}).
\tag{112}\]</span></span></p></li>
<li><p>Prior on standard deviation associated with non-neutral lineages likelihood function (one per non-neutral barcode) <span id="eq-prior_fit_var"><span class="math display">\[
\sigma^{(m)} \sim \log\mathcal{N}(\mu_{\sigma^{(m)}}, \sigma_{\sigma^{(m)}})
\tag{113}\]</span></span></p></li>
</ul>
<p>The <code>BarBay.jl</code> package includes a function <code>naive_prior</code> within the <code>stats</code> module. This function utilizes the data from the neutral lineages to determine some of the prior parameters to facilitate the inference algorithm’s numerical convergence. In particular, it defines the population mean fitness parameter <span class="math inline">\(\mu_{\bar{s}_t}\)</span> as <span id="eq-prior_meanfit_mean_data"><span class="math display">\[
\mu_{\bar{s}_t} = \frac{1}{N} \sum_{n=1}^N
-\ln\left( \frac{r_{t+1}^{(n)}}{r_t^{(n)}} \right),
\tag{114}\]</span></span> where <span class="math inline">\(N\)</span> is the number of neutral lineages and <span class="math inline">\(r_t^{(n)}\)</span> is the number of neutral lineages. In other words, it defines the mean of the prior distribution as the mean of what one naively would compute from the neutral lineages, discarding cases where the ratio diverges because the denominator <span class="math inline">\(r_t^{(n)} = 0\)</span>. For the variance parameter, we chose a value <span class="math inline">\(\sigma_{\bar{s}_t} = 0.05\)</span>.</p>
<p>Furthermore, the <code>naive_prior</code> function defines the mean of the variance parameter as the standard deviation of the log frequency ratios for the neutral lineages, i.e., <span id="eq-prior_meanfit_var_data"><span class="math display">\[
\mu_{\sigma_t} = \sqrt{
    \operatorname{Var}\left(\frac{r_{t+1}^{(n)}}{r_t^{(n)}}\right)
},
\tag{115}\]</span></span> where <span class="math inline">\(\operatorname{Var}\)</span> is the sample variance. This same value was utilized for the mean of the non-neutral barcode variance <span class="math inline">\(\mu_{\sigma^{(m)}}\)</span>. While we assigne the corresponding variances to be <span class="math inline">\(\sigma_{\sigma_t} = \sigma_{\sigma^{(m)}} = 1\)</span>.</p>
<!-- Posterior predictive checks -->
</section>
</section>
</section>
<section id="sec-ppc" class="level2">
<h2 class="anchored" data-anchor-id="sec-ppc">Posterior predictive checks</h2>
<p>Throughout the main text, we allude to the concept of posterior predictive checks as a formal way to assess the accuracy of our inference pipeline. Here, we explain the mechanics behind the computation of these credible regions, given the output of the inference.</p>
<p>Bayesian models encode what is known as a <em>generative model</em>. This statement means that in our definition of the likelihood function and the prior distribution, we, as modelers, propose a mathematical function that captures all relevant relationships between unobserved (latent) variables. Therefore, when these latent variables are input into the mathematical model, this function <em>generates</em> data that should be, in principle, indistinguishable from the real observations if the model is a good account of the underlying processes involved in the phenomena of interest. This generative model implies that once we run the inference process and update our posterior beliefs about the state of the latent variables, we can input back the inferred values to our model and generate synthetic data. Furthermore, we can repeat this process multiple times to compute the range where we expect to observe our data conditioned on the accuracy of the model.</p>
<p>For our specific scenario, recall that our objective is to infer the relative fitness of a non-neutral lineage <span class="math inline">\(s^{(m)}\)</span> along with nuisance parameters related to the population mean fitness at each point, <span class="math inline">\(\underline{\bar{s}}_t\)</span>, and the barcode frequency time series <span class="math inline">\(\underline{f}^{(m)}\)</span>. All these variables are related through our fitness model (see <a href="#sec-fitness_model">Section&nbsp;2.3</a> in the main text) <span id="eq-ppc_model"><span class="math display">\[
f_{t+1}^{(m)} = f_{t}^{(m)} \mathrm{e}^{(s^{(m)} - s_t)\tau}.
\tag{116}\]</span></span> As we saw, it is convenient to rewrite <a href="#eq-ppc_model">Equation&nbsp;116</a> as <span id="eq-ppc_logratio"><span class="math display">\[
\frac{1}{\tau}\ln\frac{f_{t+1}^{(m)}}{f_{t}^{(m)}} = (s^{(m)} - s_t).
\tag{117}\]</span></span> Written in this way, we separate the quantities we can compute from the experimental observations—the left-hand side of <a href="#eq-ppc_logratio">Equation&nbsp;117</a> can be computed from the barcode reads—from the latent variables.</p>
<p>Although we perform the joint inference over all barcodes in the present work, let us focus on the inference task for a single barcode as if it were computed independently. For a non-neutral barcode, our task consists of computing the posterior probability <span id="eq-ppc_posterior"><span class="math display">\[
\pi(\theta \mid \underline{r}^{(m)}) =
\pi(
    s^{(m)}, \sigma^{(m)}, \underline{s}_t, \underline{f}^{(m)}
    \mid \underline{r}^{(m)}
),
\tag{118}\]</span></span> where <span class="math inline">\(\theta\)</span> represents all parameters to be inferred and <span class="math inline">\(\underline{r}^{(m)}\)</span> is the vector with the barcode raw counts time series. The list of parameters are</p>
<ul>
<li><span class="math inline">\(s^{(m)}\)</span>: The barcode’s relative fitness.</li>
<li><span class="math inline">\(\sigma^{(m)}\)</span>: A nuisance parameter used in the likelihood to generate the data. This captures the expected deviation from <a href="#eq-ppc_logratio">Equation&nbsp;117</a></li>
<li><span class="math inline">\(\underline{s}_t\)</span>: The vector with all population mean fitness for each pair of adjacent time points.</li>
<li><span class="math inline">\(\underline{f}^{(m)}\)</span>: The vector with the barcode frequency time series.</li>
</ul>
<p>Furthermore, let us define a naive estimate of the barcode frequency at time <span class="math inline">\(t\)</span> as <span id="eq-ppc_naive_freq"><span class="math display">\[
\hat{f}_t^{(m)} = \frac{r^{(m)}_t}{\sum_{b=1}^B r^{(b)}_t}.
\tag{119}\]</span></span> We can compute this quantity from the data by normalizing the raw barcode counts by the sum of all barcode counts. Furthermore, we can compute a naive estimate of the log frequency ratio from the raw barcode counts as <span id="eq-ppc_naive_logfreq"><span class="math display">\[
\ln \hat{\gamma}_t^{(m)} = \ln \frac{\hat{f}_{t+1}^{(m)}}{\hat{f}_t^{(m)}}
\tag{120}\]</span></span></p>
<p>In our generative model, we assumed <span id="eq-ppc_logfreq_likelihood"><span class="math display">\[
\ln\gamma^{(m)}_t \mid \theta \sim \mathcal{N}(s^{(m)} - s_t, \sigma^{(m)}).
\tag{121}\]</span></span> This implies that once we determine the posterior distribution of our parameters, we can generate synthetic values of <span class="math inline">\(\ln \gamma^{(m)}_t\)</span> that we can then compare with the values obtained from applying <a href="#eq-ppc_naive_logfreq">Equation&nbsp;120</a> and <a href="#eq-ppc_naive_logfreq">Equation&nbsp;120</a> to the raw data.</p>
<p>In practice, to compute the posterior predictive checks, we generate multiple samples from the posterior distribution <span class="math inline">\(\pi(\theta \mid \underline{r}^{(m)})\)</span> <span id="eq-ppc_theta_samples"><span class="math display">\[
\underline{\theta} = (\theta_1, \theta_2, \ldots, \theta_N).
\tag{122}\]</span></span> With these samples in hand, the <code>BarBay.jl</code> package includes the function <code>logfreq_ratio_bc_ppc</code> for non-neutral barcodes that uses this set of posterior parameter samples to generate samples from the distribution defined in <a href="#eq-ppc_logfreq_likelihood">Equation&nbsp;121</a>. For a large-enough number of samples, we can then compute the desired percentiles—5, 68, and 95 percentiles in all figures in the main text—that are equivalent to the corresponding credible regions. In other words, the range of values of <span class="math inline">\(\ln\gamma_t^{(m)}\)</span> generated by this bootstrap process can be used to compute the region where we expect to find our raw estimates <span class="math inline">\(\ln\hat{\gamma}_t^{(m)}\)</span> with the desired probability. The package <code>BarBay.jl</code> includes an equivalent function, <code>logfreq_ratio_popmean_ppc</code>, for neutral lineages.</p>
<!-- Logistic growth simulations -->
</section>
<section id="sec-logistic" class="level2">
<h2 class="anchored" data-anchor-id="sec-logistic">Logistic growth simulation</h2>
<p>In this section, we explain the simulations used to assess the validity of our inference pipeline. Let us begin by assuming that, since the strains are grown for two full days in the experiment, having left behind the exponential phase for almost an entire day, a simple exponential growth of the form <span id="eq-logistic_expo"><span class="math display">\[
\frac{dn_i}{dt} = \lambda_i n_i,
\tag{123}\]</span></span> where <span class="math inline">\(n_i\)</span> is the number of cells of strain <span class="math inline">\(i\)</span>, and <span class="math inline">\(\lambda_i\)</span> is the corresponding growth rate is not enough. Instead, we will assume that the cells follow the logistic growth equation of the form <span id="eq-logistic_fn"><span class="math display">\[
\frac{dn_i}{dt} = \lambda_i n_i
\left( 1 - \frac{\sum_{j=1}^{N}n_i}{\kappa}\right),
\tag{124}\]</span></span> where <span class="math inline">\(\kappa\)</span> is the carrying capacity, and <span class="math inline">\(N\)</span> is the total number of strains in the culture.</p>
<p>The inference method is based on the model that assumes that the time passed between dilutions <span class="math inline">\(\tau \approx 8\)</span> generations, the change in frequency for a mutant barcode can be approximated from cycle <span class="math inline">\(t\)</span> to the next cycle <span class="math inline">\(t + 1\)</span> as <span id="eq-logistic_fitness_fn"><span class="math display">\[
f_{t+1}^{(m)} = f_t^{(m)} \mathrm{e}^{(s^{(m)} - \bar{s}_t)\tau},
\tag{125}\]</span></span> where <span class="math inline">\(s^{(m)}\)</span> is the relative fitness for strain <span class="math inline">\(i\)</span> compared to the ancestral strain and <span class="math inline">\(\bar{s}_t\)</span> is the mean fitness of the population at cycle <span class="math inline">\(t\)</span>. To test this assumption, we implemented a numerical experiment following the logistic growth model described in <a href="#eq-logistic_fn">Equation&nbsp;124</a>. <a href="#fig-SIX_logistic_growth_01">Figure&nbsp;8</a> shows an example of the deterministic trajectories for 50 labeled neutral lineages and 1000 lineages of interest. The upper red curve that dominates the culture represents the unlabeled ancestral strain included in the experimental design described in <a href="#sec-experiment">Section&nbsp;2.1</a>.</p>
<div id="fig-SIX_logistic_growth_01" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./figs/figSIX_logistic_growth_01.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;8: <strong>Logistic growth simulation over single growth cycle</strong>. The dashed line represents the neutral lineages, with the upper curve being the unlabeled neutral strain. Color curves represent the genotypes of interest colored by growth rate relative to the neutral lineage.</figcaption>
</figure>
</div>
<p>To simulate multiple growth-dilution cycles, we take the population composition at the final time point and use it to initialize a new logistic growth simulation. <a href="#fig-SIX_logistic_growth_02">Figure&nbsp;9</a> shows the resulting number of cells at the last time point of a cycle over multiple growth-dilution cycles for the genotypes in @<a href="#fig-SIX_logistic_growth_01">Figure&nbsp;8</a>. We can see that the adaptive lineages (blue curves) increase in abundance, while detrimental lineages (red curves) decrease.</p>
<div id="fig-SIX_logistic_growth_02" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./figs/figSIX_logistic_growth_02.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;9: <strong>Growth-dilution cycles for logistic growth simulation</strong>. Each point represents the final number of cells after a growth cycle for each lineage. Colors are the same as in <a href="#fig-SIX_logistic_growth_01">Figure&nbsp;8</a>.</figcaption>
</figure>
</div>
<p>In <a href="#sec-fitness_model">Section&nbsp;2.3</a>, we derive the functional form to infer the relative fitness of each lineage as <span id="eq-logistic_logfreq"><span class="math display">\[
\frac{1}{\tau}\ln \frac{f_{t+1}^{(b)}}{f_{t}^{(b)}} = (s^{(b)} - \bar{s}_t).
\tag{126}\]</span></span> <a href="#fig-SIX_logistic_growth_03">Figure&nbsp;10</a> shows the corresponding log frequency ratio curves for the logistic growth simulation. The displacement of these curves with respect to the neutral lineages determines the ground truth relative fitness value for these simulations.</p>
<div id="fig-SIX_logistic_growth_03" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./figs/figSIX_logistic_growth_03.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;10: <strong>Log frequency ratio for logistic growth simulations</strong>. The relative distance of the color curves from the black dashed line determines the relative fitness of each linage.</figcaption>
</figure>
</div>
<p>To simulate the experimental noise, we add two types of noise:</p>
<ol type="1">
<li>Poisson noise between dilutions. For this, we take the final point of the logistic growth simulation and sample a random Poisson number based on this last point to set the initial condition for the next cycle.</li>
<li>Gaussian noise when performing the measurements. When translating the underlying population composition to the number of reads, we can add a custom amount of Gaussian noise.</li>
</ol>
<p><a href="#fig-SIX_logistic_growth_04">Figure&nbsp;11</a> shows the frequency trajectories (left panels) and log frequency ratios (right panels) for a noiseless simulation (upper panels) and a simulation with added noise (lower panels). The noiseless simulation is used to determine the relative fitness for each of the lineages, which serves as the ground truth to be compared with the resulting inference.</p>
<div id="fig-SIX_logistic_growth_04" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./figs/figSIX_logistic_growth_04.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;11: <strong>Logistic growth-dilution simulations with and without noise</strong></figcaption>
</figure>
</div>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body" role="list">
<div id="ref-levy2015" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">S. F. Levy, J. R. Blundell, S. Venkataram, D. A. Petrov, D. S. Fisher, and G. Sherlock, <span>“Quantitative evolutionary dynamics using high-resolution lineage tracking,”</span> <em>Nature</em>, vol. 519, no. 7542, pp. 181–186, Feb. 2015, doi: <a href="https://doi.org/10.1038/nature14279">10.1038/nature14279</a>.</div>
</div>
<div id="ref-nguyenba2019a" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">A. N. Nguyen Ba <em>et al.</em>, <span>“High-resolution lineage tracking reveals travelling wave of adaptation in laboratory yeast,”</span> <em>Nature</em>, vol. 575, no. 7783, pp. 494–499, Nov. 2019, doi: <a href="https://doi.org/10.1038/s41586-019-1749-3">10.1038/s41586-019-1749-3</a>.</div>
</div>
<div id="ref-ascensao2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">J. A. Ascensao, K. M. Wetmore, B. H. Good, A. P. Arkin, and O. Hallatschek, <span>“Quantifying the local adaptive landscape of a nascent bacterial community,”</span> <em>Nat Commun</em>, vol. 14, no. 1, p. 248, Jan. 2023, doi: <a href="https://doi.org/10.1038/s41467-022-35677-5">10.1038/s41467-022-35677-5</a>.</div>
</div>
<div id="ref-kinsler2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">G. Kinsler, K. Geiler-Samerotte, and D. A. Petrov, <span>“Fitness variation across subtle environmental perturbations reveals local modularity and global pleiotropy of adaptation,”</span> <em>eLife</em>, vol. 9, pp. 1–52, Dec. 2020, doi: <a href="https://doi.org/10.7554/eLife.61271">10.7554/eLife.61271</a>.</div>
</div>
<div id="ref-eddy2004a" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">S. R. Eddy, <span>“What is <span>Bayesian</span> statistics?”</span> <em>Nature Biotechnology</em>, vol. 22, no. 9, pp. 1177–1178, Sep. 2004, doi: <a href="https://doi.org/10.1038/nbt0904-1177">10.1038/nbt0904-1177</a>.</div>
</div>
<div id="ref-betancourt2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">M. Betancourt, <span>“A <span>Conceptual Introduction</span> to <span>Hamiltonian Monte Carlo</span>,”</span> <em>ArXiv</em>, 2017, Available: <a href="https://arxiv.org/abs/1701.02434">https://arxiv.org/abs/1701.02434</a></div>
</div>
<div id="ref-kucukelbir2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">A. Kucukelbir, D. Tran, R. Ranganath, A. Gelman, and D. M. Blei, <span>“Automatic <span>Differentiation Variational Inference</span>,”</span> Mar. 02, 2016. <a href="https://arxiv.org/abs/1603.00788">https://arxiv.org/abs/1603.00788</a> (accessed Jul. 07, 2023).</div>
</div>
<div id="ref-efron2013a" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">B. Efron, <span>“Bayes’ <span>Theorem</span> in the 21st <span>Century</span>,”</span> <em>Science</em>, vol. 340, no. 6137, pp. 1177–1178, 2013, doi: <a href="https://doi.org/10.1126/science.1236536">10.1126/science.1236536</a>.</div>
</div>
<div id="ref-kingma2014" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">D. P. Kingma and M. Welling, <span>“Auto-<span>Encoding Variational Bayes</span>,”</span> May 01, 2014. <a href="http://arxiv.org/abs/1312.6114">http://arxiv.org/abs/1312.6114</a> (accessed Nov. 21, 2022).</div>
</div>
<div id="ref-gottwald2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">S. Gottwald and D. A. Braun, <span>“The two kinds of free energy and the <span>Bayesian</span> revolution,”</span> <em>PLOS Computational Biology</em>, vol. 16, no. 12, p. e1008420, Dec. 2020, doi: <a href="https://doi.org/10.1371/journal.pcbi.1008420">10.1371/journal.pcbi.1008420</a>.</div>
</div>
<div id="ref-ge2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">H. Ge, K. Xu, and Z. Ghahramani, <span>“Turing: <span>A Language</span> for <span>Flexible Probabilistic Inference</span>,”</span> in <em>Proceedings of the <span>Twenty-First International Conference</span> on <span>Artificial Intelligence</span> and <span>Statistics</span></em>, <span>PMLR</span>, Mar. 2018, pp. 1682–1690. Accessed: Jul. 26, 2023. [Online]. Available: <a href="https://proceedings.mlr.press/v84/ge18b.html">https://proceedings.mlr.press/v84/ge18b.html</a></div>
</div>
<div id="ref-morey2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">R. D. Morey, R. Hoekstra, J. N. Rouder, M. D. Lee, and E.-J. Wagenmakers, <span>“The fallacy of placing confidence in confidence intervals,”</span> <em>Psychon Bull Rev</em>, vol. 23, no. 1, pp. 103–123, Feb. 2016, doi: <a href="https://doi.org/10.3758/s13423-015-0947-8">10.3758/s13423-015-0947-8</a>.</div>
</div>
<div id="ref-betancourt2013" class="csl-entry" role="listitem">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">M. J. Betancourt and M. Girolami, <span>“Hamiltonian <span>Monte Carlo</span> for <span>Hierarchical Models</span>,”</span> Dec. 03, 2013. <a href="http://arxiv.org/abs/1312.0906">http://arxiv.org/abs/1312.0906</a> (accessed Jul. 20, 2023).</div>
</div>
<div id="ref-kussell2013" class="csl-entry" role="listitem">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline">E. Kussell, <span>“Evolution in <span>Microbes</span>,”</span> <em>Annu. Rev. Biophys</em>, vol. 42, pp. 493–514, 2013, doi: <a href="https://doi.org/10.1146/annurev-biophys-083012-130320">10.1146/annurev-biophys-083012-130320</a>.</div>
</div>
<div id="ref-Dekel2005" class="csl-entry" role="listitem">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline">E. Dekel and U. Alon, <span>“Optimality and evolutionary tuning of the expression level of a protein,”</span> <em>Nature</em>, vol. 436, no. 7050, pp. 588–592, Jul. 2005, doi: <a href="https://doi.org/10.1038/nature03842">10.1038/nature03842</a>.</div>
</div>
<div id="ref-maeda2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline">T. Maeda <em>et al.</em>, <span>“High-throughput laboratory evolution reveals evolutionary constraints in <span>Escherichia</span> coli,”</span> <em>Nature Communications</em>, vol. 11, no. 1, p. 5970, Dec. 2020, doi: <a href="https://doi.org/10.1038/s41467-020-19713-w">10.1038/s41467-020-19713-w</a>.</div>
</div>
<div id="ref-good2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[17] </div><div class="csl-right-inline">B. H. Good, M. J. Mcdonald, J. E. Barrick, R. E. Lenski, and M. M. Desai, <span>“The dynamics of molecular evolution over 60,000 generations,”</span> <em>Nature</em>, 2017, doi: <a href="https://doi.org/10.1038/nature24287">10.1038/nature24287</a>.</div>
</div>
<div id="ref-jagdish2022" class="csl-entry" role="listitem">
<div class="csl-left-margin">[18] </div><div class="csl-right-inline">T. Jagdish and A. N. Nguyen Ba, <span>“Microbial experimental evolution in a massively multiplexed and high-throughput era,”</span> <em>Current Opinion in Genetics &amp; Development</em>, vol. 75, p. 101943, Aug. 2022, doi: <a href="https://doi.org/10.1016/j.gde.2022.101943">10.1016/j.gde.2022.101943</a>.</div>
</div>
<div id="ref-yang2022" class="csl-entry" role="listitem">
<div class="csl-left-margin">[19] </div><div class="csl-right-inline">D. Yang <em>et al.</em>, <span>“Lineage tracing reveals the phylodynamics, plasticity, and paths of tumor evolution,”</span> <em>Cell</em>, vol. 185, no. 11, pp. 1905–1923.e25, May 2022, doi: <a href="https://doi.org/10.1016/j.cell.2022.04.015">10.1016/j.cell.2022.04.015</a>.</div>
</div>
<div id="ref-gelman2010" class="csl-entry" role="listitem">
<div class="csl-left-margin">[20] </div><div class="csl-right-inline">A. Gelman and C. R. Shalizi, <span>“Philosophy and the practice of <span>Bayesian</span> statistics,”</span> <em>Statistics</em>, vol. math.ST, no. 1996, p. 36, 2010, doi: <a href="https://doi.org/10.1111/j.2044-8317.2011.02037.x">10.1111/j.2044-8317.2011.02037.x</a>.</div>
</div>
<div id="ref-vanderplas2014" class="csl-entry" role="listitem">
<div class="csl-left-margin">[21] </div><div class="csl-right-inline">J. VanderPlas, <span>“Frequentism and <span>Bayesianism</span>: <span class="nocase">A Python-driven Primer</span>,”</span> <em>ArXiv</em>, pp. 1–9, Nov. 2014, Available: <a href="http://arxiv.org/abs/1411.5018">http://arxiv.org/abs/1411.5018</a></div>
</div>
<div id="ref-nuzzo2014" class="csl-entry" role="listitem">
<div class="csl-left-margin">[22] </div><div class="csl-right-inline">R. Nuzzo, <span>“Statistical errors,”</span> <em>Nature</em>, vol. 506, 2014.</div>
</div>
<div id="ref-li2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">[23] </div><div class="csl-right-inline">F. Li, J. Tarkington, and G. Sherlock, <span>“Fit-<span>Seq2</span>.0: <span>An Improved Software</span> for <span>High-Throughput Fitness Measurements Using Pooled Competition Assays</span>,”</span> <em>J Mol Evol</em>, Mar. 2023, doi: <a href="https://doi.org/10.1007/s00239-023-10098-0">10.1007/s00239-023-10098-0</a>.</div>
</div>
<div id="ref-lovell2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[24] </div><div class="csl-right-inline">D. R. Lovell, X.-Y. Chua, and A. McGrath, <span>“Counts: An outstanding challenge for log-ratio analysis of compositional data in the molecular biosciences,”</span> <em>NAR Genomics and Bioinformatics</em>, vol. 2, no. 2, p. lqaa040, Jun. 2020, doi: <a href="https://doi.org/10.1093/nargab/lqaa040">10.1093/nargab/lqaa040</a>.</div>
</div>
<div id="ref-gelman2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[25] </div><div class="csl-right-inline">A. Gelman, D. Simpson, and M. Betancourt, <span>“The <span>Prior Can Often Only Be Understood</span> in the <span>Context</span> of the <span>Likelihood</span>,”</span> <em>Entropy</em>, vol. 19, no. 10, p. 555, Oct. 2017, doi: <a href="https://doi.org/10.3390/e19100555">10.3390/e19100555</a>.</div>
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>