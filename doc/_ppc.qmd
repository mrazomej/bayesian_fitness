---
editor:
    render-on-save: true
bibliography: references.bib
csl: ieee.csl
---

## Posterior predictive checks {#sec-ppc}

Throughout the main text, we allude to the concept of posterior predictive
checks as a formal way to assess the accuracy of our inference pipeline. Here,
we explain the mechanics behind the computation of these credible regions, given
the output of the inference.

Bayesian models encode what is known as a *generative model*. This statement
means that in our definition of the likelihood function and the prior
distribution, we, as modelers, propose a mathematical function that captures all
relevant relationships between unobserved (latent) variables. Therefore, when
these latent variables are input into the mathematical model, this function
*generates* data that should be, in principle, indistinguishable from the real
observations if the model is a good account of the underlying processes involved
in the phenomena of interest. This generative model implies that once we run the
inference process and update our posterior beliefs about the state of the latent
variables, we can input back the inferred values to our model and generate
synthetic data. Furthermore, we can repeat this process multiple times to
compute the range where we expect to observe our data conditioned on the
accuracy of the model.

For our specific scenario, recall that our objective is to infer the relative
fitness of a non-neutral lineage $s^{(m)}$ along with nuisance parameters
related to the population mean fitness at each point, $\underline{\bar{s}}_t$,
and the barcode frequency time series $\underline{f}^{(m)}$. All these variables
are related through our fitness model (see @sec-fitness_model in the main text)
$$
f_{t+1}^{(m)} = f_{t}^{(m)} \mathrm{e}^{(s^{(m)} - s_t)\tau}.
$${#eq-ppc_model}
As we saw, it is convenient to rewrite @eq-ppc_model as
$$
\frac{1}{\tau}\ln\frac{f_{t+1}^{(m)}}{f_{t}^{(m)}} = (s^{(m)} - s_t).
$${#eq-ppc_logratio}
Written in this way, we separate the quantities we can compute from the 
experimental observations---the left-hand side of @eq-ppc_logratio can be 
computed from the barcode reads---from the latent variables.

Although we perform the joint inference over all barcodes in the present work,
let us focus on the inference task for a single barcode as if it were computed
independently. For a non-neutral barcode, our task consists of computing the 
posterior probability
$$
\pi(\theta \mid \underline{r}^{(m)}) =
\pi(
    s^{(m)}, \sigma^{(m)}, \underline{s}_t, \underline{f}^{(m)}
    \mid \underline{r}^{(m)}
),
$${#eq-ppc_posterior}
where $\theta$ represents all parameters to be inferred and
$\underline{r}^{(m)}$ is the vector with the barcode raw counts time series. The
list of parameters are

- $s^{(m)}$: The barcode's relative fitness.
- $\sigma^{(m)}$: A nuisance parameter used in the likelihood to generate the
data. This captures the expected deviation from @eq-ppc_logratio
- $\underline{s}_t$: The vector with all population mean fitness for each pair
of adjacent time points.
- $\underline{f}^{(m)}$: The vector with the barcode frequency time series.

Furthermore, let us define a naive estimate of the barcode frequency at time $t$
as 
$$
\hat{f}_t^{(m)} = \frac{r^{(m)}_t}{\sum_{b=1}^B r^{(b)}_t}.
$${#eq-ppc_naive_freq}
We can compute this quantity from the data by normalizing the raw barcode counts
by the sum of all barcode counts. Furthermore, we can compute a naive estimate
of the log frequency ratio from the raw barcode counts as
$$
\ln \hat{\gamma}_t^{(m)} = \ln \frac{\hat{f}_{t+1}^{(m)}}{\hat{f}_t^{(m)}}
$${#eq-ppc_naive_logfreq}

In our generative model, we assumed
$$
\ln\gamma^{(m)}_t \mid \theta \sim \mathcal{N}(s^{(m)} - s_t, \sigma^{(m)}).
$${#eq-ppc_logfreq_likelihood}
This implies that once we determine the posterior distribution of our
parameters, we can generate synthetic values of $\ln \gamma^{(m)}_t$ that we can
then compare with the values obtained from applying @eq-ppc_naive_logfreq and
@eq-ppc_naive_logfreq to the raw data. 

In practice, to compute the posterior predictive checks, we generate multiple
samples from the posterior distribution $\pi(\theta \mid \underline{r}^{(m)})$ 
$$
\underline{\theta} = (\theta_1, \theta_2, \ldots, \theta_N).
$${#eq-ppc_theta_samples}
With these samples in hand, the `BayesFitness.jl` package includes the function
`logfreq_ratio_bc_ppc` for non-neutral barcodes that uses this set of posterior
parameter samples to generate samples from the distribution defined in
@eq-ppc_logfreq_likelihood. For a large-enough number of samples, we can then
compute the desired percentiles---5, 68, and 95 percentiles in all figures in
the main text---that are equivalent to the corresponding credible regions. In
other words, the range of values of $\ln\gamma_t^{(m)}$ generated by this
bootstrap process can be used to compute the region where we expect to find our
raw estimates $\ln\hat{\gamma}_t^{(m)}$ with the desired probability. The
package `BayesFitness.jl` includes an equivalent function,
`logfreq_ratio_popmean_ppc`, for neutral lineages.